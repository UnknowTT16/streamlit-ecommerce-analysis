# ==============================================================================
# 1. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“
# ==============================================================================
import streamlit as st
import pandas as pd
import numpy as np
from stqdm import stqdm
import os

# (å…³é”®) è§£å†³ KMeans å†…å­˜æ³„æ¼è­¦å‘Š
os.environ['OMP_NUM_THREADS'] = '1'

# åˆ†æåº“
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense, Input
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from deep_translator import GoogleTranslator

# å¯è§†åŒ–åº“
import plotly.graph_objects as go
import plotly.express as px

# ==============================================================================
# 2. é…ç½®ä¸è¾…åŠ©å‡½æ•°
# ==============================================================================
@st.cache_data
def convert_df_to_csv(df):
    return df.to_csv(index=False).encode('utf-8-sig')

# --- (å·²å‡çº§) åˆ†æå‡½æ•°ï¼šæ—¶é—´åºåˆ—é¢„æµ‹ (LSTM) ---
@st.cache_data
def perform_lstm_forecast(_df): # ä½¿ç”¨ _df é¿å…ä¸ streamlit å†…éƒ¨å˜é‡å†²çª
    st.write("æ­£åœ¨ä¸ºæ·±åº¦å­¦ä¹ å‡†å¤‡æ•°æ®...")
    sales_ts = _df.groupby('Date')['Amount'].sum().asfreq('D', fill_value=0)
    sales_values = sales_ts.values.reshape(-1, 1)
    
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_values = scaler.fit_transform(sales_values)

    def create_dataset(data, look_back=7):
        X, y = [], []
        for i in range(len(data) - look_back):
            X.append(data[i:(i + look_back), 0])
            y.append(data[i + look_back, 0])
        return np.array(X), np.array(y)

    look_back = 7
    X, y = create_dataset(scaled_values, look_back)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    st.write("æ­£åœ¨æ„å»ºå¹¶è®­ç»ƒ LSTM æ¨¡å‹ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    model = Sequential([Input(shape=(look_back, 1)), LSTM(50), Dense(1)])
    model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(X, y, epochs=20, batch_size=32, verbose=0)

    st.write("æ­£åœ¨é¢„æµ‹æœªæ¥...")
    last_days_scaled = scaled_values[-look_back:]
    current_input = np.reshape(last_days_scaled, (1, look_back, 1))
    future_predictions_scaled = []
    for _ in range(30):
        next_pred_scaled = model.predict(current_input, verbose=0)
        future_predictions_scaled.append(next_pred_scaled[0, 0])
        new_pred_reshaped = np.reshape(next_pred_scaled, (1, 1, 1))
        current_input = np.append(current_input[:, 1:, :], new_pred_reshaped, axis=1)
    
    future_predictions = scaler.inverse_transform(np.array(future_predictions_scaled).reshape(-1, 1))
    
    last_date = sales_ts.index[-1]
    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30)
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=sales_ts.index, y=sales_ts.values, name='å†å²é”€å”®é¢', line=dict(color='royalblue', width=2), fill='tozeroy', fillcolor='rgba(65, 105, 225, 0.2)'))
    fig.add_trace(go.Scatter(x=future_dates, y=future_predictions.flatten(), name='LSTM é¢„æµ‹é”€å”®é¢', line=dict(color='darkorange', dash='dash', width=2), fill='tozeroy', fillcolor='rgba(255, 140, 0, 0.2)'))
    fig.update_layout(title='æœªæ¥30å¤©é”€å”®é¢æ·±åº¦å­¦ä¹ é¢„æµ‹ (LSTMæ¨¡å‹)', xaxis_title='æ—¥æœŸ', yaxis_title='é”€å”®é¢')
    return fig

# ... (å…¶ä»–è¾…åŠ©å‡½æ•°ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ) ...
def perform_product_clustering(df):
    # ...
    st.write("æ­£åœ¨æŒ‰å•†å“èšåˆæ•°æ®å¹¶è¿›è¡Œèšç±»åˆ†æ...")
    required_cols = ['SKU', 'Amount', 'Qty', 'Order ID']
    if not all(col in df.columns for col in required_cols):
        st.error(f"èšç±»åˆ†æå¤±è´¥ï¼šç¼ºå°‘å¿…è¦çš„åˆ—ã€‚éœ€è¦: {', '.join(required_cols)}")
        return None, None
    product_agg_df = df.groupby('SKU').agg(total_amount=('Amount', 'sum'), total_qty=('Qty', 'sum'), order_count=('Order ID', 'nunique')).reset_index()
    features_to_cluster = ['total_amount', 'total_qty', 'order_count']
    features = product_agg_df[features_to_cluster]
    scaler = StandardScaler(); features_scaled = scaler.fit_transform(features)
    kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)
    product_agg_df['cluster'] = kmeans.fit_predict(features_scaled)
    cluster_summary = product_agg_df.groupby('cluster')[features_to_cluster].mean().sort_values(by='total_amount', ascending=False)
    hot_product_cluster_id = cluster_summary.index[0]
    hot_products = product_agg_df[product_agg_df['cluster'] == hot_product_cluster_id].sort_values(by='total_amount', ascending=False)
    return cluster_summary, hot_products

@st.cache_resource
def get_translator(target_lang):
    return GoogleTranslator(source='en', target=target_lang)

def translate_dataframe(_df, target_langs):
    # ...
    st.write("æ­£åœ¨ç¿»è¯‘éé—é¡¹ç›®æè¿°...")
    df_translated = _df.copy()
    stqdm.pandas()
    for lang in target_langs:
        col_name = f'Description_{lang.upper()}'
        translator = get_translator(lang)
        def translate_cell(text):
            return translator.translate(text) if isinstance(text, str) and text.strip() else ""
        df_translated[col_name] = df_translated['Description EN'].progress_apply(translate_cell)
    return df_translated

def find_review_column(df):
    # ...
    priority_cols = ['reviews.text', 'review_text', 'content', 'comment', 'review']
    for p_col in priority_cols:
        if p_col in df.columns and df[p_col].dropna().astype(str).str.strip().any(): return p_col
    possible_cols = [col for col in df.columns if any(key in str(col).lower() for key in ['text', 'review', 'content', 'comment'])]
    if possible_cols:
        string_cols = [col for col in possible_cols if df[col].dtype == 'object']
        if string_cols:
            return max(string_cols, key=lambda col: df[col].dropna().astype(str).str.len().mean())
    object_cols = df.select_dtypes(include=['object']).columns
    if not object_cols.empty:
        for col in object_cols:
            if df[col].dropna().astype(str).str.strip().any(): return col
    return None

def perform_sentiment_analysis(reviews_df):
    # ...
    st.write("æ­£åœ¨å¯¹è¯„è®ºæ•°æ®è¿›è¡Œæƒ…æ„Ÿåˆ†æ...")
    def sentiment_to_rating(sentiment):
        if sentiment >= 0.5: return 5
        elif sentiment >= 0.05: return 4
        elif sentiment > -0.05: return 3
        elif sentiment > -0.5: return 2
        else: return 1
    analyzer = SentimentIntensityAnalyzer()
    has_rating_col = 'rating' in reviews_df.columns
    review_column_name = find_review_column(reviews_df)
    if review_column_name is None:
        st.error("é”™è¯¯: æœªèƒ½åœ¨è¯„è®ºæ–‡ä»¶ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬åˆ—ã€‚")
        return None
    st.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°è¯„è®ºåˆ—ä¸º: '{review_column_name}'")
    reviews_df[review_column_name] = reviews_df[review_column_name].astype(str)
    reviews_df.dropna(subset=[review_column_name], inplace=True)
    reviews_df = reviews_df[reviews_df[review_column_name].str.strip() != 'None'].copy()
    reviews_df['sentiment'] = reviews_df[review_column_name].apply(lambda text: analyzer.polarity_scores(text)['compound'])
    if not has_rating_col:
        st.info("æœªåœ¨æ•°æ®ä¸­æ‰¾åˆ° 'rating' åˆ—ï¼Œå°†æ ¹æ®æƒ…æ„Ÿåˆ†æ•°è‡ªåŠ¨ä¼°ç®—æ˜Ÿçº§ã€‚")
        reviews_df['rating'] = reviews_df['sentiment'].apply(sentiment_to_rating)
    reviews_df.rename(columns={review_column_name: 'review_text'}, inplace=True)
    return reviews_df

def create_category_sales_plot(df):
    # ...
    st.write("æ­£åœ¨ç”Ÿæˆå„äº§å“ç±»åˆ«é”€å”®å¯¹æ¯”å›¾...")
    category_means = df.groupby('Category')['Amount'].mean().sort_values(ascending=False).reset_index()
    fig = px.bar(category_means, x='Category', y='Amount', color='Category', text_auto='.2f', labels={'Category': 'äº§å“ç±»åˆ«', 'Amount': 'å¹³å‡é”€å”®é¢ (Amount)'}, title='å„äº§å“ç±»åˆ«å¹³å‡é”€å”®é¢å¯¹æ¯”')
    fig.update_layout(width=800, height=500, xaxis_title_font_size=14, yaxis_title_font_size=14, title_font_size=18, template='plotly_white', showlegend=False)
    fig.update_traces(textposition='outside', textfont_size=12)
    return fig


# ==============================================================================
# 3. Streamlit ç”¨æˆ·ç•Œé¢å¸ƒå±€
# ==============================================================================

st.set_page_config(layout="wide")
st.title('ğŸ“ˆ è·¨å¢ƒé€‰å“ä¸é”€å”®æ•°æ®åˆ†æå·¥å…·')

with st.sidebar:
    st.header("ğŸ“‚ ä¸Šä¼ æ‚¨çš„æ•°æ®")
    uploaded_amazon = st.file_uploader('ä¸Šä¼  Amazon é”€å”®æŠ¥å‘Š (CSV)', type='csv')
    uploaded_unesco = st.file_uploader('ä¸Šä¼  UNESCO éé—æ•°æ® (CSV)', type='csv')
    uploaded_reviews = st.file_uploader('ä¸Šä¼  Amazon è¯„è®ºæ•°æ® (å¯é€‰)', type=['csv', 'parquet'])

if uploaded_amazon and uploaded_unesco:
    # --- æ•°æ®åŠ è½½ä¸å¥å£®æ€§æ¸…æ´— ---
    try:
        # åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸æŒ‡å®šdtypeï¼Œè®©pandasè‡ªåŠ¨æ¨æ–­ï¼Œåç»­å†å¤„ç†
        amazon_df = pd.read_csv(uploaded_amazon, on_bad_lines='skip')
        unesco_df = pd.read_csv(uploaded_unesco, on_bad_lines='skip')
    except Exception as e:
        st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    else:
        st.success("Amazon å’Œ UNESCO æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
        
        with st.status("âš™ï¸ æ­£åœ¨æ¸…æ´—å’Œé€‚é…æ•°æ®...", expanded=True) as status:
            # ... (æ™ºèƒ½é‡å‘½åé€»è¾‘ä¿æŒä¸å˜)
            if 'Total Sales' in amazon_df.columns: amazon_df.rename(columns={'Total Sales': 'Amount'}, inplace=True); st.write("âœ”ï¸ 'Total Sales' -> 'Amount'")
            if 'Product' in amazon_df.columns: amazon_df.rename(columns={'Product': 'SKU'}, inplace=True); st.write("âœ”ï¸ 'Product' -> 'SKU'")
            if 'Qty' not in amazon_df.columns and 'Quantity' in amazon_df.columns: amazon_df.rename(columns={'Quantity': 'Qty'}, inplace=True); st.write("âœ”ï¸ 'Quantity' -> 'Qty'")
            if 'Order ID' not in amazon_df.columns and 'Order_ID' in amazon_df.columns: amazon_df.rename(columns={'Order_ID': 'Order ID'}, inplace=True); st.write("âœ”ï¸ 'Order_ID' -> 'Order ID'")
            
            required_cols = ["Amount", "Category", "Date", "Status", "SKU", "Order ID", "Qty"]
            missing_cols = [col for col in required_cols if col not in amazon_df.columns]
            
            if missing_cols:
                status.update(label="æ•°æ®æ¸…æ´—å¤±è´¥!", state="error", expanded=True)
                st.error(f"ä¸Šä¼ çš„ Amazon æ–‡ä»¶ä¸­ç¼ºå°‘å…³é”®åˆ—: {', '.join(missing_cols)}")
            else:
                amazon_df.dropna(subset=["Amount", "Category", "Date"], inplace=True)
                
                # æ¶ˆé™¤æ—¥æœŸæ ¼å¼è­¦å‘Š
                try:
                    amazon_df["Date"] = pd.to_datetime(amazon_df["Date"], format='%m-%d-%y')
                    st.write("âœ”ï¸ æ—¥æœŸæ ¼å¼æˆåŠŸåŒ¹é…: MM-DD-YYã€‚")
                except ValueError:
                    st.write("âš ï¸ æ—¥æœŸæ ¼å¼ä¸åŒ¹é… MM-DD-YYï¼Œå›é€€åˆ°è‡ªåŠ¨è§£æ...")
                    amazon_df["Date"] = pd.to_datetime(amazon_df["Date"], errors='coerce')
                
                # ... (åç»­æ¸…æ´—é€»è¾‘ä¿æŒä¸å˜)
                amazon_df["Amount"] = pd.to_numeric(amazon_df["Amount"], errors='coerce')
                valid_statuses = ["Shipped", "Shipped - Delivered to Buyer", "Completed", "Pending", "Cancelled"]
                amazon_df = amazon_df[amazon_df["Status"].isin(valid_statuses)]
                amazon_df.dropna(subset=['Date', 'Amount', 'SKU', 'Order ID', 'Qty'], inplace=True)
                all_categories = amazon_df['Category'].unique()
                noné—_products = amazon_df[amazon_df['Category'].str.contains('|'.join(all_categories), case=False, na=False)]
                status.update(label="æ•°æ®æ¸…æ´—ä¸é€‚é…å®Œæˆ!", state="complete", expanded=False)

                # --- åˆ›å»ºé€‰é¡¹å¡ ---
                tabs = ["ğŸ§  LSTMé”€å”®é¢„æµ‹", "ğŸ›ï¸ å“ç±»è¡¨ç°", "ğŸ”¥ çƒ­é”€å“èšç±»", "ğŸ’¬ æƒ…æ„Ÿåˆ†æ", "ğŸŒ éé—æè¿°ç¿»è¯‘"]
                tab1, tab2, tab3, tab4, tab5 = st.tabs(tabs)

                # --- (å…³é”®ä¿®æ”¹) å°†åŸtab1çš„ARIMAé¢„æµ‹æ›¿æ¢ä¸ºLSTMé¢„æµ‹ ---
                with tab1:
                    st.header("é”€å”®é¢æ·±åº¦å­¦ä¹ é¢„æµ‹ (LSTM)")
                    st.markdown("ä½¿ç”¨ LSTM æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ ¹æ®å†å²é”€å”®æ•°æ®é¢„æµ‹æœªæ¥30å¤©çš„é”€å”®è¶‹åŠ¿ã€‚")
                    # ä½¿ç”¨ç¼“å­˜æ¥é¿å…æ¯æ¬¡åˆ‡æ¢Tabéƒ½é‡æ–°è®­ç»ƒæ¨¡å‹
                    lstm_forecast_fig = perform_lstm_forecast(amazon_df)
                    st.plotly_chart(lstm_forecast_fig, use_container_width=True)

                # --- (å…¶ä»–Tabä¿æŒä¸å˜) ---
                with tab2:
                    # ...
                    st.header("äº§å“ç±»åˆ«é”€å”®è¡¨ç°")
                    st.markdown("å¯¹æ¯”ä¸åŒäº§å“ç±»åˆ«çš„å¹³å‡é”€å”®é¢ã€‚")
                    with st.spinner('æ­£åœ¨ç”Ÿæˆç±»åˆ«å¯¹æ¯”å›¾...'):
                        category_fig = create_category_sales_plot(noné—_products)
                        st.plotly_chart(category_fig)

                with tab3:
                    # ...
                    st.header("çƒ­é”€å•†å“èšç±»åˆ†æ")
                    st.markdown("é€šè¿‡K-Meansèšç±»ï¼Œæ ¹æ®æ€»é”€å”®é¢ã€æ€»é”€é‡å’Œè®¢å•æ•°æ‰¾å‡ºçƒ­é—¨å•†å“ã€‚")
                    with st.spinner('æ­£åœ¨è¿›è¡Œèšç±»åˆ†æ...'):
                        cluster_summary, hot_products = perform_product_clustering(amazon_df)
                        if cluster_summary is not None:
                            st.subheader("å„å•†å“ç°‡ç‰¹å¾å‡å€¼"); st.dataframe(cluster_summary)
                            st.subheader("ğŸ”¥ çƒ­é”€å•†å“åˆ—è¡¨"); st.dataframe(hot_products)
                            st.download_button("ä¸‹è½½çƒ­é”€å•†å“åˆ—è¡¨ (CSV)", convert_df_to_csv(hot_products), "hot_products.csv", "text/csv")
                
                with tab4:
                    # ...
                    st.header("å®¢æˆ·è¯„è®ºæƒ…æ„Ÿåˆ†æ")
                    if uploaded_reviews:
                        # ...
                        sentiment_df = None
                        file_name = uploaded_reviews.name
                        try:
                            if file_name.endswith('.parquet'):
                                st.info(f"æ­£åœ¨åŠ è½½é¢„å¤„ç†çš„ Parquet æ–‡ä»¶: '{file_name}'...")
                                sentiment_df = pd.read_parquet(uploaded_reviews)
                                st.success("Parquet æ–‡ä»¶åŠ è½½æˆåŠŸï¼")
                            elif file_name.endswith('.csv'):
                                st.info(f"æ­£åœ¨å®æ—¶åˆ†æä¸Šä¼ çš„ CSV æ–‡ä»¶: '{file_name}'...")
                                with st.spinner('è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...'):
                                    reviews_df = pd.read_csv(uploaded_reviews)
                                    sentiment_df = perform_sentiment_analysis(reviews_df)
                                st.success("CSV æ–‡ä»¶åˆ†æå®Œæˆï¼")
                            if sentiment_df is not None:
                                st.subheader("æŒ‰æ˜Ÿçº§ç­›é€‰è¯„è®º")
                                rating_range = st.slider('é€‰æ‹©è¦æ˜¾ç¤ºçš„æ˜Ÿçº§è¯„åˆ†èŒƒå›´:', 1, 5, (4, 5))
                                min_val, max_val = rating_range
                                filtered_reviews = sentiment_df[(sentiment_df['rating'] >= min_val) & (sentiment_df['rating'] <= max_val)]
                                st.markdown(f"**æ˜¾ç¤º {len(filtered_reviews)} æ¡è¯„åˆ†ä¸º {min_val} åˆ° {max_val} æ˜Ÿçš„è¯„è®º**" if min_val != max_val else f"**æ˜¾ç¤º {len(filtered_reviews)} æ¡è¯„åˆ†ä¸º {min_val} æ˜Ÿçš„è¯„è®º**")
                                st.dataframe(filtered_reviews[['rating', 'review_text', 'sentiment']])
                                st.subheader("æƒ…æ„Ÿåˆ†æ•°ç»Ÿè®¡")
                                avg_sentiment_filtered = filtered_reviews['sentiment'].mean() if not filtered_reviews.empty else 0
                                avg_sentiment_all = sentiment_df['sentiment'].mean()
                                col1, col2 = st.columns(2)
                                metric_label = f"æ‰€é€‰è¯„è®º ({min_val}-{max_val} æ˜Ÿ) çš„å¹³å‡æƒ…æ„Ÿåˆ†" if min_val != max_val else f"æ‰€é€‰è¯„è®º ({min_val} æ˜Ÿ) çš„å¹³å‡æƒ…æ„Ÿåˆ†"
                                col1.metric(metric_label, f"{avg_sentiment_filtered:.2f}")
                                col2.metric("æ‰€æœ‰è¯„è®ºçš„å¹³å‡æƒ…æ„Ÿåˆ†", f"{avg_sentiment_all:.2f}")
                        except Exception as e:
                            st.error(f"å¤„ç†è¯„è®ºæ–‡ä»¶æ—¶å‡ºé”™: {e}")
                    else:
                        st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼ ä¸€ä¸ªè¯„è®ºæ–‡ä»¶ (æ”¯æŒ .csv æˆ– .parquet æ ¼å¼) ä»¥è¿›è¡Œåˆ†æã€‚")

                with tab5:
                    # ...
                    st.header("UNESCO éé—é¡¹ç›®æè¿°å¤šè¯­è¨€ç¿»è¯‘")
                    st.markdown("å°†è‹±æ–‡æè¿°ç¿»è¯‘æˆå…¶ä»–è¯­è¨€ï¼Œä»¥æ”¯æŒä¸åŒå¸‚åœºçš„å–å®¶ã€‚")
                    available_langs = {'å¾·è¯­': 'de', 'æ³•è¯­': 'fr', 'è¥¿ç­ç‰™è¯­': 'es', 'æ—¥è¯­': 'ja', 'ä¿„è¯­': 'ru', 'ä¸­æ–‡': 'zh-cn'}
                    selected_langs_names = st.multiselect('é€‰æ‹©ç›®æ ‡è¯­è¨€:', list(available_langs.keys()), default=['å¾·è¯­', 'æ³•è¯­', 'ä¸­æ–‡'])
                    target_lang_codes = [available_langs[name] for name in selected_langs_names]
                    if st.button('å¼€å§‹ç¿»è¯‘'):
                        if not target_lang_codes:
                            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ç§ç›®æ ‡è¯­è¨€ã€‚")
                        else:
                            unesco_subset = unesco_df.head(20)
                            with st.spinner('ç¿»è¯‘è¿›è¡Œä¸­ï¼Œè¯·ç¨å€™...'):
                                translated_df = translate_dataframe(unesco_subset, target_langs=target_lang_codes)
                                st.success("ç¿»è¯‘å®Œæˆï¼")
                                st.dataframe(translated_df)
                                st.download_button("ä¸‹è½½ç¿»è¯‘åçš„æ•°æ® (CSV)", convert_df_to_csv(translated_df), "unesco_translated.csv", "text/csv")
else:
    st.info("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ï¼è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼  Amazon å’Œ UNESCO çš„ CSV æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")