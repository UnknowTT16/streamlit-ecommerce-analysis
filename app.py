# ==============================================================================
# 1. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“
# ==============================================================================
import streamlit as st
import pandas as pd
import numpy as np
from stqdm import stqdm
import os
import warnings

# (å…³é”®) è§£å†³ KMeans å†…å­˜æ³„æ¼è­¦å‘Š
os.environ['OMP_NUM_THREADS'] = '1'

# åˆ†æåº“
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense, Input
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from deep_translator import GoogleTranslator
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from pandas.errors import SettingWithCopyWarning, DtypeWarning

# (æ–°å¢) è¯äº‘å›¾å’Œç»˜å›¾åº“
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

# (æ–°å¢) æŠ‘åˆ¶ç‰¹å®šçš„Pandasè­¦å‘Šï¼Œç¾åŒ–è¾“å‡º
warnings.filterwarnings('ignore', category=SettingWithCopyWarning)
warnings.filterwarnings('ignore', category=DtypeWarning)

import plotly.graph_objects as go
import plotly.express as px

# ==============================================================================
# 2. é…ç½®ä¸è¾…åŠ©å‡½æ•°
# ==============================================================================
@st.cache_data
def convert_df_to_csv(df):
    return df.to_csv(index=False).encode('utf-8-sig')

def load_uploaded_file(uploaded_file, dtype_spec=None):
    if uploaded_file is None: return None
    file_name = uploaded_file.name
    try:
        if file_name.endswith('.parquet'): return pd.read_parquet(uploaded_file)
        elif file_name.endswith('.csv'): return pd.read_csv(uploaded_file, on_bad_lines='skip', dtype=dtype_spec)
        else: st.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_name}ã€‚"); return None
    except Exception as e:
        st.error(f"è¯»å–æ–‡ä»¶ '{file_name}' æ—¶å‡ºé”™: {e}"); return None

@st.cache_data
def generate_wordcloud(text_series):
    if text_series.empty: return None
    full_text = " ".join(review for review in text_series.dropna())
    wordcloud = WordCloud(stopwords=STOPWORDS, background_color="white", width=1200, height=600, colormap='viridis').generate(full_text)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis("off")
    plt.tight_layout(pad=0)
    return fig

@st.cache_data
def perform_semantic_matching(_amazon_df, _unesco_df):
    if 'text_for_matching' not in _amazon_df.columns or not _amazon_df['text_for_matching'].notna().any():
        _amazon_df['text_for_matching'] = _amazon_df['Category'].fillna('')
    _unesco_df['text_for_matching'] = _unesco_df['Description EN'].fillna('')
    corpus = pd.concat([_amazon_df['text_for_matching'].fillna(''), _unesco_df['text_for_matching'].fillna('')], ignore_index=True)
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(corpus)
    amazon_matrix = tfidf_matrix[:len(_amazon_df)]
    unesco_matrix = tfidf_matrix[len(_amazon_df):]
    similarity_matrix = cosine_similarity(amazon_matrix, unesco_matrix)
    return similarity_matrix, list(_unesco_df['Title EN'])

@st.cache_data
def perform_lstm_forecast(_df):
    sales_ts = _df.groupby('Date')['Amount'].sum().asfreq('D', fill_value=0)
    sales_values = sales_ts.values.reshape(-1, 1)
    scaler = MinMaxScaler(feature_range=(0, 1)); scaled_values = scaler.fit_transform(sales_values)
    def create_dataset(data, look_back=7):
        X, y = [], [];
        for i in range(len(data) - look_back): X.append(data[i:(i + look_back), 0]); y.append(data[i + look_back, 0])
        return np.array(X), np.array(y)
    look_back = 7; X, y = create_dataset(scaled_values, look_back); X = np.reshape(X, (X.shape[0], X.shape[1], 1))
    model = Sequential([Input(shape=(look_back, 1)), LSTM(50), Dense(1)]); model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(X, y, epochs=20, batch_size=32, verbose=0)
    last_days_scaled = scaled_values[-look_back:]; current_input = np.reshape(last_days_scaled, (1, look_back, 1)); future_predictions_scaled = []
    for _ in range(30):
        next_pred_scaled = model.predict(current_input, verbose=0); future_predictions_scaled.append(next_pred_scaled[0, 0])
        new_pred_reshaped = np.reshape(next_pred_scaled, (1, 1, 1)); current_input = np.append(current_input[:, 1:, :], new_pred_reshaped, axis=1)
    future_predictions = scaler.inverse_transform(np.array(future_predictions_scaled).reshape(-1, 1))
    last_date = sales_ts.index[-1]; future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30); fig = go.Figure()
    fig.add_trace(go.Scatter(x=sales_ts.index, y=sales_ts.values, name='å†å²é”€å”®é¢', line=dict(color='royalblue', width=2), fill='tozeroy', fillcolor='rgba(65, 105, 225, 0.2)'))
    fig.add_trace(go.Scatter(x=future_dates, y=future_predictions.flatten(), name='LSTM é¢„æµ‹é”€å”®é¢', line=dict(color='darkorange', dash='dash', width=2), fill='tozeroy', fillcolor='rgba(255, 140, 0, 0.2)'))
    fig.update_layout(title='æœªæ¥30å¤©é”€å”®é¢æ·±åº¦å­¦ä¹ é¢„æµ‹ (LSTMæ¨¡å‹)', xaxis_title='æ—¥æœŸ', yaxis_title='é”€å”®é¢'); return fig
    
@st.cache_data
def perform_product_clustering(_df):
    required_cols = ['SKU', 'Amount', 'Qty', 'Order ID']
    if not all(col in _df.columns for col in _df.columns):
        return None, None, f"èšç±»åˆ†æå¤±è´¥ï¼šç¼ºå°‘å¿…è¦çš„åˆ—ã€‚éœ€è¦: {', '.join(required_cols)}"
    product_agg_df = _df.groupby('SKU').agg(total_amount=('Amount', 'sum'), total_qty=('Qty', 'sum'), order_count=('Order ID', 'nunique')).reset_index()
    features_to_cluster = ['total_amount', 'total_qty', 'order_count']
    features = product_agg_df[features_to_cluster]
    scaler = StandardScaler(); features_scaled = scaler.fit_transform(features)
    kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)
    product_agg_df['cluster'] = kmeans.fit_predict(features_scaled)
    cluster_summary = product_agg_df.groupby('cluster')[features_to_cluster].mean().sort_values(by='total_amount', ascending=False)
    hot_product_cluster_id = cluster_summary.index[0]
    hot_products = product_agg_df[product_agg_df['cluster'] == hot_product_cluster_id].sort_values(by='total_amount', ascending=False)
    return cluster_summary, hot_products, None

def find_review_column(df):
    priority_cols = ['reviews.text', 'review_text', 'content', 'comment', 'review']
    for p_col in priority_cols:
        if p_col in df.columns and df[p_col].dropna().astype(str).str.strip().any(): return p_col
    possible_cols = [col for col in df.columns if any(key in str(col).lower() for key in ['text', 'review', 'content', 'comment'])]
    if possible_cols:
        string_cols = [col for col in possible_cols if df[col].dtype == 'object']
        if string_cols:
            return max(string_cols, key=lambda col: df[col].dropna().astype(str).str.len().mean())
    object_cols = df.select_dtypes(include=['object']).columns
    if not object_cols.empty:
        for col in object_cols:
            if df[col].dropna().astype(str).str.strip().any(): return col
    return None

@st.cache_data
def perform_sentiment_analysis(reviews_df):
    def sentiment_to_rating(sentiment):
        if sentiment >= 0.5: return 5
        elif sentiment >= 0.05: return 4
        elif sentiment > -0.05: return 3
        elif sentiment > -0.5: return 2
        else: return 1
    analyzer = SentimentIntensityAnalyzer()
    has_rating_col = 'rating' in reviews_df.columns
    review_column_name = find_review_column(reviews_df)
    if review_column_name is None:
        return None, "é”™è¯¯: æœªèƒ½åœ¨è¯„è®ºæ–‡ä»¶ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬åˆ—ã€‚"
    reviews_df[review_column_name] = reviews_df[review_column_name].astype(str)
    reviews_df.dropna(subset=[review_column_name], inplace=True)
    reviews_df = reviews_df[reviews_df[review_column_name].str.strip() != 'None'].copy()
    stqdm.pandas(desc="æ­£åœ¨è®¡ç®—æƒ…æ„Ÿåˆ†æ•°") 
    reviews_df['sentiment'] = reviews_df[review_column_name].progress_apply(lambda text: analyzer.polarity_scores(text)['compound'])
    if not has_rating_col:
        reviews_df['rating'] = reviews_df['sentiment'].apply(sentiment_to_rating)
    reviews_df.rename(columns={review_column_name: 'review_text'}, inplace=True)
    return reviews_df, None

@st.cache_data
def create_category_sales_plot(_df):
    category_means = _df.groupby('Category')['Amount'].mean().sort_values(ascending=False).reset_index()
    fig = px.bar(category_means, x='Category', y='Amount', color='Category', text_auto='.2f', labels={'Category': 'äº§å“ç±»åˆ«', 'Amount': 'å¹³å‡é”€å”®é¢ (Amount)'}, title='å„äº§å“ç±»åˆ«å¹³å‡é”€å”®é¢å¯¹æ¯”')
    fig.update_layout(width=800, height=500, xaxis_title_font_size=14, yaxis_title_font_size=14, title_font_size=18, template='plotly_white', showlegend=False)
    fig.update_traces(textposition='outside', textfont_size=12)
    return fig

@st.cache_resource
def get_translator(target_lang):
    return GoogleTranslator(source='en', target=target_lang)

@st.cache_data
def translate_page(_df, lang):
    df_copy = _df.copy()
    translator = get_translator(lang)
    col_name = f'Description_{lang.upper()}'
    df_copy[col_name] = _df['Description EN'].apply(lambda x: translator.translate(x) if isinstance(x, str) and x.strip() else "")
    return df_copy

# ==============================================================================
# 3. Streamlit ç”¨æˆ·ç•Œé¢å¸ƒå±€
# ==============================================================================

st.set_page_config(layout="wide")
st.title('ğŸ“ˆ è·¨å¢ƒé€‰å“ä¸é”€å”®æ•°æ®åˆ†æå·¥å…· ')

with st.sidebar:
    st.header("ğŸ“‚ ä¸Šä¼ æ‚¨çš„æ•°æ®")
    st.info("æç¤ºï¼šä¸ºäº†è·å¾—æ›´å¿«çš„ä¸Šä¼ å’Œå¤„ç†é€Ÿåº¦ï¼Œæ¨èæ‚¨å°†å¤§çš„CSVæ–‡ä»¶è½¬æ¢ä¸ºParquetæ ¼å¼ã€‚")
    uploaded_amazon = st.file_uploader('1. ä¸Šä¼  Amazon é”€å”®æŠ¥å‘Š', type=['csv', 'parquet'])
    uploaded_metadata = st.file_uploader('2. ä¸Šä¼ å•†å“å…ƒæ•°æ® (å¯é€‰)', type=['csv', 'parquet'])
    uploaded_unesco = st.file_uploader('3. ä¸Šä¼  UNESCO éé—æ•°æ®', type=['csv', 'parquet'])
    uploaded_reviews = st.file_uploader('4. ä¸Šä¼  Amazon è¯„è®ºæ•°æ® (å¯é€‰)', type=['csv', 'parquet'])

if uploaded_amazon and uploaded_unesco:
    dtype_spec = {'ASIN': str, 'asin': str}
    amazon_df = load_uploaded_file(uploaded_amazon, dtype_spec=dtype_spec)
    unesco_df = load_uploaded_file(uploaded_unesco)
    reviews_df_loaded = load_uploaded_file(uploaded_reviews)

    if amazon_df is not None and unesco_df is not None:
        st.success("ä¸»æ•°æ®æ–‡ä»¶åŠ è½½æˆåŠŸï¼")
        
        with st.status("âš™ï¸ æ­£åœ¨æ¸…æ´—å’Œé€‚é…æ•°æ®...", expanded=True) as status:
            if uploaded_metadata:
                metadata_df = load_uploaded_file(uploaded_metadata, dtype_spec=dtype_spec)
                if metadata_df is not None:
                    st.write(f"--- å‘ç°å…ƒæ•°æ®æ–‡ä»¶ '{uploaded_metadata.name}'ï¼Œå‡†å¤‡è¿›è¡ŒåŠ¨æ€åˆå¹¶ ---")
                    sales_key_col, metadata_key_col = next((c for c in ['ASIN','asin'] if c in amazon_df.columns),None), next((c for c in ['asin','ASIN'] if c in metadata_df.columns),None)
                    desc_candidates = ['about_product','description','title','product_name','name','item_name']
                    metadata_desc_col = next((c for c in desc_candidates if c in metadata_df.columns), None)
                    if sales_key_col and metadata_key_col and metadata_desc_col:
                        st.write(f"âœ”ï¸ è‡ªåŠ¨æ¢æµ‹æˆåŠŸ: ä½¿ç”¨ '{sales_key_col}' å’Œ '{metadata_key_col}' ä½œä¸ºå…±åŒé”®ã€‚")
                        st.write(f"âœ”ï¸ å°†ä½¿ç”¨ '{metadata_desc_col}' ä½œä¸ºå•†å“æè¿°æ¥æºã€‚")
                        amazon_df[sales_key_col] = amazon_df[sales_key_col].astype(str).str.strip()
                        metadata_df[metadata_key_col] = metadata_df[metadata_key_col].astype(str).str.strip()
                        metadata_subset = metadata_df[[metadata_key_col, metadata_desc_col]].drop_duplicates(subset=[metadata_key_col])
                        amazon_df = pd.merge(amazon_df, metadata_subset, left_on=sales_key_col, right_on=metadata_key_col, how='left')
                        total, matched = len(amazon_df), amazon_df[metadata_desc_col].notna().sum()
                        rate = (matched/total)*100 if total > 0 else 0
                        st.write(f"ğŸ“Š æ•°æ®åˆå¹¶å®Œæˆï¼åŒ¹é…æˆåŠŸç‡: **{rate:.2f}%** ({matched}/{total} æ¡è®°å½•)ã€‚")
                        fallback = amazon_df['Category'].fillna('')+' '+amazon_df.get('Style',pd.Series(index=amazon_df.index,dtype=str)).fillna('')
                        amazon_df['text_for_matching'] = amazon_df[metadata_desc_col].fillna(fallback)
                        st.write("--> å·²ä¸ºæ‰€æœ‰å•†å“åˆ›å»ºæœ€ç»ˆæè¿°æ–‡æœ¬ 'text_for_matching'ã€‚")
                    else:
                        st.warning("âš ï¸ æ— æ³•å®Œæˆåˆå¹¶ï¼Œå°†ä½¿ç”¨åŸºç¡€ä¿¡æ¯è¿›è¡Œåˆ†æã€‚")
                        amazon_df['text_for_matching'] = amazon_df['Category'].fillna('')
            else:
                st.write("--- æœªé€‰æ‹©å…ƒæ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨åŸºç¡€ä¿¡æ¯è¿›è¡Œåˆ†æ ---")
                amazon_df['text_for_matching'] = amazon_df['Category'].fillna('')+' '+amazon_df.get('Style',pd.Series(index=amazon_df.index,dtype=str)).fillna('')

            for old, new in {'Total Sales':'Amount','Product':'SKU','Quantity':'Qty','Order_ID':'Order ID'}.items():
                if old in amazon_df.columns: amazon_df.rename(columns={old:new},inplace=True)
            
            req_cols = ["Amount","Category","Date","Status","SKU","Order ID","Qty"]
            missing = [c for c in req_cols if c not in amazon_df.columns]
            if missing:
                status.update(label="æ•°æ®æ¸…æ´—å¤±è´¥!", state="error", expanded=True)
                st.error(f"Amazon æ–‡ä»¶ä¸­ç¼ºå°‘å…³é”®åˆ—: {', '.join(missing)}")
            else:
                amazon_df.dropna(subset=["Amount", "Category", "Date"], inplace=True)
                try: amazon_df["Date"] = pd.to_datetime(amazon_df["Date"], format='%m-%d-%y')
                except ValueError: amazon_df["Date"] = pd.to_datetime(amazon_df["Date"], errors='coerce')
                amazon_df["Amount"] = pd.to_numeric(amazon_df["Amount"], errors='coerce')
                amazon_df = amazon_df[amazon_df["Status"].isin(["Shipped","Shipped - Delivered to Buyer","Completed","Pending","Cancelled"])]
                amazon_df.dropna(subset=['Date','Amount','SKU','Order ID','Qty'],inplace=True)
                all_cats = amazon_df['Category'].unique()
                noné—_products = amazon_df[amazon_df['Category'].str.contains('|'.join(all_cats), case=False, na=False)]
                status.update(label="æ•°æ®æ¸…æ´—ä¸é€‚é…å®Œæˆ!", state="complete", expanded=False)

                with st.spinner('æ­£åœ¨è¿›è¡Œåˆæ¬¡æ•°æ®é¢„å¤„ç†å’Œæ¨¡å‹è®¡ç®—ï¼Œè¯·ç¨å€™... (æ­¤è¿‡ç¨‹ä»…åœ¨é¦–æ¬¡åŠ è½½æ—¶è¿è¡Œ)'):
                    cluster_summary, hot_products, cluster_error = perform_product_clustering(amazon_df)
                    keywords = ['craft','textile','embroidery','weaving','costume','dress','heritage product','handicraft']
                    relevant_unesco = unesco_df[unesco_df['Description EN'].str.contains('|'.join(keywords),case=False,na=False)]
                    
                    cosine_sim, unesco_titles = (None, None)
                    if not relevant_unesco.empty:
                        cosine_sim, unesco_titles = perform_semantic_matching(amazon_df, relevant_unesco)
                    
                    sentiment_df, sentiment_error = (None, None)
                    if reviews_df_loaded is not None:
                        sentiment_df, sentiment_error = perform_sentiment_analysis(reviews_df_loaded)
                
                st.success("æ ¸å¿ƒè®¡ç®—å®Œæˆï¼ç°åœ¨æ‚¨å¯ä»¥å¿«é€Ÿæµè§ˆæ‰€æœ‰åˆ†æç»“æœã€‚")

                tabs = ["ğŸ”— è¯­ä¹‰å…³è”æ¨è", "ğŸ§  LSTMé”€å”®é¢„æµ‹", "ğŸ›ï¸ å“ç±»è¡¨ç°", "ğŸ”¥ çƒ­é”€å“èšç±»", "ğŸ’¬ æƒ…æ„Ÿåˆ†æ", "ğŸŒ éé—æè¿°ç¿»è¯‘"]
                tab_sm, tab1, tab2, tab3, tab4, tab5 = st.tabs(tabs)

                with tab_sm:
                    st.header("æ–‡åŒ–å…³è”ä¸å•†å“æ¨è")
                    if cosine_sim is not None and unesco_titles is not None:
                        st.subheader("ğŸ’¡ åœºæ™¯ä¸€: ä¸ºæ‚¨çš„çƒ­é”€å“å¯»æ‰¾æ–‡åŒ–çµæ„Ÿ")
                        if hot_products is not None and not hot_products.empty:
                            top_hot = hot_products.head(20)
                            sel_sku = st.selectbox('ä»æ‚¨çš„Top 20çƒ­é”€å•†å“ä¸­é€‰æ‹©ä¸€ä¸ª:', top_hot['SKU'], format_func=lambda x: f"{x} (æ€»é”€å”®é¢: {top_hot.loc[top_hot['SKU']==x,'total_amount'].iloc[0]:.2f})")
                            p_indices = amazon_df.index[amazon_df['SKU']==sel_sku].tolist()
                            if p_indices:
                                p_idx = p_indices[0]
                                s_scores = sorted(list(enumerate(cosine_sim[p_idx])), key=lambda x:x[1], reverse=True)
                                top_5 = [i[0] for i in s_scores[0:5]]
                                st.write(f"ä¸å•†å“ **'{sel_sku}'** æœ€ç›¸å…³çš„5ä¸ªéé—é¡¹ç›®æ˜¯:")
                                for idx in top_5:
                                    st.markdown(f"- **{unesco_titles[idx]}** (ç›¸ä¼¼åº¦: {cosine_sim[p_idx,idx]:.4f})")
                        else: st.warning("æœªèƒ½è¯†åˆ«å‡ºçƒ­é”€å•†å“åˆ—è¡¨ã€‚")
                        st.subheader("ğŸš€ åœºæ™¯äºŒ: æ ¹æ®æ–‡åŒ–å…ƒç´ åå‘å¯»æ‰¾æ½œåŠ›å•†å“")
                        sel_heritage = st.selectbox('ä»ç›¸å…³çš„éé—é¡¹ç›®ä¸­é€‰æ‹©ä¸€ä¸ª:', unesco_titles)
                        if sel_heritage:
                            h_idx = unesco_titles.index(sel_heritage)
                            s_scores_h = sorted(list(enumerate(cosine_sim[:,h_idx])),key=lambda x:x[1],reverse=True)
                            top_10 = [i[0] for i in s_scores_h[:10]]
                            st.write(f"ä¸éé—é¡¹ç›® **'{sel_heritage}'** æœ€ç›¸ä¼¼çš„Top 10åœ¨å”®å•†å“æ˜¯:")
                            rec_prods = amazon_df.iloc[top_10][['SKU','Amount','Category','text_for_matching']]
                            st.dataframe(rec_prods)
                    else: st.warning("æœªåœ¨UNESCOæ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸å…³çš„éé—é¡¹ç›®ã€‚")

                with tab1:
                    st.header("é”€å”®é¢æ·±åº¦å­¦ä¹ é¢„æµ‹ (LSTM)")
                    with st.spinner('æ­£åœ¨ç”ŸæˆLSTMé¢„æµ‹å›¾è¡¨...'):
                        lstm_fig = perform_lstm_forecast(amazon_df)
                    st.plotly_chart(lstm_fig, use_container_width=True)

                with tab2:
                    st.header("äº§å“ç±»åˆ«é”€å”®è¡¨ç°")
                    with st.spinner('æ­£åœ¨ç”Ÿæˆå“ç±»è¡¨ç°å›¾...'):
                        cat_fig = create_category_sales_plot(noné—_products)
                    st.plotly_chart(cat_fig)

                with tab3:
                    st.header("çƒ­é”€å•†å“èšç±»åˆ†æ")
                    if cluster_error: st.error(cluster_error)
                    elif cluster_summary is not None and hot_products is not None:
                        st.subheader("å„å•†å“ç°‡ç‰¹å¾å‡å€¼"); st.dataframe(cluster_summary)
                        st.subheader(f"ğŸ”¥ çƒ­é”€å•†å“åˆ—è¡¨ (å…± {len(hot_products)} ä¸ª)")
                        if len(hot_products)>20:
                            st.dataframe(hot_products.head(20))
                            if st.checkbox('æ˜¾ç¤ºæ‰€æœ‰çƒ­é”€å•†å“',key='show_all_hot'):
                                st.dataframe(hot_products)
                        else: st.dataframe(hot_products)
                        st.download_button("ä¸‹è½½çƒ­é”€å•†å“åˆ—è¡¨ (CSV)", convert_df_to_csv(hot_products), "hot_products.csv", "text/csv")

                with tab4:
                    st.header("å®¢æˆ·è¯„è®ºæƒ…æ„Ÿåˆ†æ")
                    if uploaded_reviews:
                        if sentiment_error:
                            st.error(sentiment_error)
                        # (***å…³é”®ä¿®å¤ç‚¹***) ä½¿ç”¨æ­£ç¡®çš„å˜é‡å `sentiment_df`
                        elif sentiment_df is not None:
                            st.subheader("æŒ‰æ˜Ÿçº§ç­›é€‰è¯„è®º")
                            rating_range = st.slider('é€‰æ‹©æ˜Ÿçº§èŒƒå›´:',1,5,(4,5))
                            min_r, max_r = rating_range
                            filtered_reviews = sentiment_df[(sentiment_df['rating']>=min_r)&(sentiment_df['rating']<=max_r)]
                            st.markdown(f"**æ˜¾ç¤º {len(filtered_reviews)} æ¡è¯„åˆ†ä¸º {min_r} åˆ° {max_r} æ˜Ÿçš„è¯„è®º**")
                            st.dataframe(filtered_reviews[['rating','review_text','sentiment']])
                            st.subheader("æƒ…æ„Ÿåˆ†æ•°ç»Ÿè®¡")
                            avg_filtered = filtered_reviews['sentiment'].mean() if not filtered_reviews.empty else 0
                            avg_all = sentiment_df['sentiment'].mean()
                            c1,c2 = st.columns(2)
                            c1.metric(f"æ‰€é€‰è¯„è®º ({min_r}-{max_r} æ˜Ÿ) çš„å¹³å‡æƒ…æ„Ÿåˆ†", f"{avg_filtered:.2f}")
                            c2.metric("æ‰€æœ‰è¯„è®ºçš„å¹³å‡æƒ…æ„Ÿåˆ†", f"{avg_all:.2f}")
                            
                            st.subheader(f"â­ {min_r}-{max_r} æ˜Ÿè¯„è®ºå…³é”®è¯è¯äº‘å›¾")
                            if not filtered_reviews.empty:
                                with st.spinner('æ­£åœ¨æ ¹æ®æ‚¨é€‰æ‹©çš„è¯„è®ºç”Ÿæˆè¯äº‘å›¾...'):
                                    wordcloud_fig = generate_wordcloud(filtered_reviews['review_text'])
                                    st.pyplot(wordcloud_fig)
                            else:
                                st.info("å½“å‰ç­›é€‰èŒƒå›´å†…æ²¡æœ‰è¯„è®ºå¯ç”¨äºç”Ÿæˆè¯äº‘å›¾ã€‚")
                    else:
                        st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼ è¯„è®ºæ–‡ä»¶ (æ”¯æŒ .csv æˆ– .parquet æ ¼å¼)ã€‚")

                with tab5:
                    st.header("UNESCO éé—é¡¹ç›®æè¿°å¤šè¯­è¨€ç¿»è¯‘")
                    st.markdown("å°†è‹±æ–‡æè¿°åˆ†é¡µæ˜¾ç¤ºï¼Œå¹¶æŒ‰éœ€è¿›è¡Œå³æ—¶ç¿»è¯‘ã€‚")
                    page_size, total_rows = 20, len(unesco_df)
                    total_pages = (total_rows//page_size)+(1 if total_rows%page_size>0 else 0) if total_rows>0 else 1
                    page_num = st.number_input(f'é€‰æ‹©é¡µç  (å…± {total_pages} é¡µ)',min_value=1,max_value=total_pages,value=1)
                    start, end = (page_num-1)*page_size, page_num*page_size
                    unesco_page = unesco_df.iloc[start:end]
                    st.markdown(f"**æ­£åœ¨æ˜¾ç¤ºç¬¬ {page_num} é¡µ, ç¬¬ {start+1} åˆ° {min(end,total_rows)} æ¡è®°å½•**")
                    display_df = unesco_page[['Title EN','Description EN']]
                    with st.expander("ğŸŒ ç‚¹å‡»è¿™é‡Œå±•å¼€ç¿»è¯‘é€‰é¡¹"):
                        langs = {'ä¸­æ–‡':'zh-CN','å¾·è¯­':'de','æ³•è¯­':'fr','è¥¿ç­ç‰™è¯­':'es','æ—¥è¯­':'ja','ä¿„è¯­':'ru'}
                        lang_name = st.selectbox('é€‰æ‹©ç›®æ ‡è¯­è¨€:', list(langs.keys()))
                        if lang_name:
                            lang_code = langs[lang_name]
                            if st.button(f"å°†å½“å‰é¡µç¿»è¯‘æˆ {lang_name}"):
                                with st.spinner('æ­£åœ¨ç¿»è¯‘å½“å‰é¡µé¢...'):
                                    try:
                                        trans_page = translate_page(unesco_page, lang_code)
                                        trans_col = f'Description_{lang_code.upper()}'
                                        if trans_col in trans_page.columns:
                                            display_df = trans_page[['Title EN','Description EN',trans_col]]
                                    except Exception as e: st.error(f"ç¿»è¯‘å¤±è´¥: {e}")
                    st.dataframe(display_df)
else:
    st.info("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ï¼è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼  Amazon å’Œ UNESCO çš„æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")