{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e759b5-69ea-4e65-bd85-59ba6ba90e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 1: å¯¼å…¥æ‰€æœ‰åº“å¹¶å®šä¹‰åŒ…å«æ‰€æœ‰åŠŸèƒ½çš„æœ€ç»ˆç‰ˆä¸»åˆ†æžå‡½æ•°\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# è§£å†³ KMeans å†…å­˜æ³„æ¼è­¦å‘Š (å¿…é¡»åœ¨å¯¼å…¥ KMeans ä¹‹å‰è®¾ç½®)\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# åˆ†æžåº“\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "\n",
    "# å¯è§†åŒ–åº“\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# NLP & äº¤äº’åº“\n",
    "from deep_translator import GoogleTranslator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "# --- (å…³é”®ä¿®å¤) è¿™æ˜¯ä¸€ä¸ªä¿®å¤äº†è¯­æ³•é”™è¯¯çš„ã€æ ‡å‡†çš„å¤šè¡Œå‡½æ•° ---\n",
    "def sentiment_to_rating(sentiment):\n",
    "    if sentiment >= 0.5:\n",
    "        return 5\n",
    "    elif sentiment >= 0.05:\n",
    "        return 4\n",
    "    elif sentiment > -0.05:\n",
    "        return 3\n",
    "    elif sentiment > -0.5:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# --- å°†æ‰€æœ‰åˆ†æžæ­¥éª¤å°è£…åˆ°ä¸»å‡½æ•°ä¸­ ---\n",
    "def run_analysis(sales_file, unesco_file, reviews_file):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"--- æ­£åœ¨ä½¿ç”¨é”€å”®æ–‡ä»¶: '{sales_file}' ---\")\n",
    "    if unesco_file: print(f\"--- æ­£åœ¨ä½¿ç”¨UNESCOæ–‡ä»¶: '{unesco_file}' ---\")\n",
    "    if reviews_file: print(f\"--- æ­£åœ¨ä½¿ç”¨è¯„è®ºæ–‡ä»¶: '{reviews_file}' ---\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # ==================== 1. æ•°æ®åŠ è½½ä¸Žæ¸…æ´— ====================\n",
    "    try:\n",
    "        print(\"--- æ­£åœ¨åŠ è½½ä¸Žæ¸…æ´—æ•°æ® ---\")\n",
    "        amazon_df = pd.read_csv(sales_file, dtype={23: str}, on_bad_lines='skip')\n",
    "        if unesco_file:\n",
    "            unesco_df = pd.read_csv(unesco_file, encoding=\"utf-8-sig\", on_bad_lines='skip')\n",
    "        \n",
    "        # æ™ºèƒ½é‡å‘½å\n",
    "        if 'Total Sales' in amazon_df.columns: amazon_df.rename(columns={'Total Sales': 'Amount'}, inplace=True)\n",
    "        if 'Product' in amazon_df.columns: amazon_df.rename(columns={'Product': 'SKU'}, inplace=True)\n",
    "        if 'Qty' not in amazon_df.columns and 'Quantity' in amazon_df.columns: amazon_df.rename(columns={'Quantity': 'Qty'}, inplace=True)\n",
    "        if 'Order ID' not in amazon_df.columns and 'Order_ID' in amazon_df.columns: amazon_df.rename(columns={'Order_ID': 'Order ID'}, inplace=True)\n",
    "        \n",
    "        required_cols = [\"Amount\", \"Category\", \"Date\", \"Status\", \"SKU\", \"Order ID\", \"Qty\"]\n",
    "        if any(col not in amazon_df.columns for col in required_cols): raise ValueError(f\"æ–‡ä»¶ '{sales_file}' ç¼ºå°‘å¿…éœ€çš„åˆ—ã€‚\")\n",
    "        \n",
    "        amazon_df.dropna(subset=[\"Amount\", \"Category\", \"Date\"], inplace=True)\n",
    "        try:\n",
    "            amazon_df[\"Date\"] = pd.to_datetime(amazon_df[\"Date\"], format='%m-%d-%y')\n",
    "        except ValueError:\n",
    "            amazon_df[\"Date\"] = pd.to_datetime(amazon_df[\"Date\"], errors='coerce')\n",
    "            \n",
    "        amazon_df[\"Amount\"] = pd.to_numeric(amazon_df[\"Amount\"], errors='coerce')\n",
    "        valid_statuses = [\"Shipped\", \"Shipped - Delivered to Buyer\", \"Completed\", \"Pending\", \"Cancelled\"]\n",
    "        amazon_df = amazon_df[amazon_df[\"Status\"].isin(valid_statuses)]\n",
    "        amazon_df.dropna(subset=['Date', 'Amount', 'SKU', 'Order ID', 'Qty'], inplace=True)\n",
    "        \n",
    "        all_categories = amazon_df['Category'].unique()\n",
    "        noné—_products = amazon_df[amazon_df['Category'].str.contains('|'.join(all_categories), case=False, na=False)]\n",
    "        \n",
    "        print(\"âœ… æ•°æ®åŠ è½½å’Œæ¸…æ´—å®Œæˆï¼\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}\"); return\n",
    "\n",
    "    # ==================== 2. è¿‡æ»¤ä¸Žæ˜ å°„ ====================\n",
    "    print(\"\\n--- æ­£åœ¨è¿›è¡Œè¿‡æ»¤ä¸Žæ˜ å°„ ---\")\n",
    "    if unesco_file:\n",
    "        keywords = ['craft', 'textile', 'embroidery', 'weaving', 'costume', 'dress', 'heritage product', 'handicraft']\n",
    "        relevant_unesco = unesco_df[unesco_df['Description EN'].str.contains('|'.join(keywords), case=False, na=False)]\n",
    "        print(f\"ç›¸å…³éžé—æ•°é‡: {len(relevant_unesco)}\")\n",
    "    print(f\"æ½œåœ¨éžé—äº§å“è®¢å•: {len(noné—_products)}\\n\")\n",
    "\n",
    "    # ==================== 3. æ—¶é—´åºåˆ—é¢„æµ‹ (LSTM) ====================\n",
    "    print(\"\\n--- ðŸ§  æ­£åœ¨è¿›è¡Œ LSTM æ·±åº¦å­¦ä¹ é¢„æµ‹ ---\")\n",
    "    try:\n",
    "        sales_ts = amazon_df.groupby('Date')['Amount'].sum().asfreq('D', fill_value=0)\n",
    "        sales_values = sales_ts.values.reshape(-1, 1)\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_values = scaler.fit_transform(sales_values)\n",
    "\n",
    "        def create_dataset(data, look_back=7):\n",
    "            X, y = [], []\n",
    "            for i in range(len(data) - look_back):\n",
    "                X.append(data[i:(i + look_back), 0])\n",
    "                y.append(data[i + look_back, 0])\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "        look_back = 7\n",
    "        X, y = create_dataset(scaled_values, look_back)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "        model = Sequential([Input(shape=(look_back, 1)), LSTM(50), Dense(1)])\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        \n",
    "        print(\"æ­£åœ¨è®­ç»ƒæ¨¡åž‹...\")\n",
    "        model.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "        print(\"æ­£åœ¨é¢„æµ‹æœªæ¥...\")\n",
    "        last_days_scaled = scaled_values[-look_back:]\n",
    "        current_input = np.reshape(last_days_scaled, (1, look_back, 1))\n",
    "        future_predictions_scaled = []\n",
    "        for _ in range(30):\n",
    "            next_pred_scaled = model.predict(current_input, verbose=0)\n",
    "            future_predictions_scaled.append(next_pred_scaled[0, 0])\n",
    "            new_pred_reshaped = np.reshape(next_pred_scaled, (1, 1, 1))\n",
    "            current_input = np.append(current_input[:, 1:, :], new_pred_reshaped, axis=1)\n",
    "        \n",
    "        future_predictions = scaler.inverse_transform(np.array(future_predictions_scaled).reshape(-1, 1))\n",
    "        \n",
    "        last_date = sales_ts.index[-1]\n",
    "        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30)\n",
    "        \n",
    "        fig_lstm = go.Figure()\n",
    "        fig_lstm.add_trace(go.Scatter(x=sales_ts.index, y=sales_ts.values, name='åŽ†å²é”€å”®é¢', line=dict(color='royalblue', width=2), fill='tozeroy', fillcolor='rgba(65, 105, 225, 0.2)'))\n",
    "        fig_lstm.add_trace(go.Scatter(x=future_dates, y=future_predictions.flatten(), name='LSTM é¢„æµ‹é”€å”®é¢', line=dict(color='darkorange', dash='dash', width=2), fill='tozeroy', fillcolor='rgba(255, 140, 0, 0.2)'))\n",
    "        fig_lstm.update_layout(title='æœªæ¥30å¤©é”€å”®é¢æ·±åº¦å­¦ä¹ é¢„æµ‹ (LSTMæ¨¡åž‹)')\n",
    "        fig_lstm.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LSTM é¢„æµ‹å¤±è´¥: {e}\")\n",
    "\n",
    "    # ==================== 4. ç±»åˆ«é”€å”®å¯è§†åŒ– ====================\n",
    "    print(\"\\n--- ðŸ›ï¸ æ­£åœ¨ç”Ÿæˆå“ç±»è¡¨çŽ°å›¾ ---\")\n",
    "    category_means = noné—_products.groupby('Category')['Amount'].mean().sort_values(ascending=False).reset_index()\n",
    "    fig_bar = px.bar(category_means, x='Category', y='Amount', color='Category', text_auto='.2f', title='å„äº§å“ç±»åˆ«å¹³å‡é”€å”®é¢å¯¹æ¯”')\n",
    "    fig_bar.update_layout(width=800, height=500, showlegend=False); fig_bar.show()\n",
    "\n",
    "    # ==================== 5. å•†å“èšç±»åˆ†æž ====================\n",
    "    print(\"\\n--- ðŸ”¥ æ­£åœ¨è¿›è¡Œå•†å“èšç±»åˆ†æž ---\")\n",
    "    try:\n",
    "        product_agg_df = amazon_df.groupby('SKU').agg(total_amount=('Amount', 'sum'), total_qty=('Qty', 'sum'), order_count=('Order ID', 'nunique')).reset_index()\n",
    "        features_to_cluster = ['total_amount', 'total_qty', 'order_count']; features = product_agg_df[features_to_cluster]\n",
    "        scaler = StandardScaler(); features_scaled = scaler.fit_transform(features)\n",
    "        kmeans = KMeans(n_clusters=3, n_init=10, random_state=42); product_agg_df.loc[:, 'cluster'] = kmeans.fit_predict(features_scaled)\n",
    "        cluster_summary = product_agg_df.groupby('cluster')[features_to_cluster].mean().sort_values(by='total_amount', ascending=False)\n",
    "        hot_product_cluster_id = cluster_summary.index[0]; hot_products = product_agg_df[product_agg_df['cluster'] == hot_product_cluster_id].sort_values(by='total_amount', ascending=False)\n",
    "        \n",
    "        print(\"\\næ¯ä¸ªå•†å“ç°‡çš„ç‰¹å¾å‡å€¼:\")\n",
    "        display(cluster_summary)\n",
    "        print(\"\\næŽ’åå‰10çš„çƒ­é”€å•†å“:\")\n",
    "        display(hot_products.head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å•†å“èšç±»å¤±è´¥: {e}\")\n",
    "\n",
    "    # ==================== 6. æƒ…æ„Ÿåˆ†æž ====================\n",
    "    if reviews_file:\n",
    "        print(\"\\n--- ðŸ’¬ æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æž ---\")\n",
    "        try:\n",
    "            def find_review_column(df):\n",
    "                priority_cols = ['reviews.text', 'review_text', 'content', 'comment', 'review']\n",
    "                for p_col in priority_cols:\n",
    "                    if p_col in df.columns and df[p_col].dropna().astype(str).str.strip().any(): return p_col\n",
    "                object_cols = df.select_dtypes(include=['object']).columns\n",
    "                if not object_cols.empty:\n",
    "                    return max(object_cols, key=lambda col: df[col].dropna().astype(str).str.len().mean())\n",
    "                return None\n",
    "            \n",
    "            reviews_df = pd.read_csv(reviews_file)\n",
    "            review_column_name = find_review_column(reviews_df)\n",
    "            if review_column_name is None: raise ValueError(\"æœªèƒ½è‡ªåŠ¨æ£€æµ‹åˆ°æ–‡æœ¬åˆ—ã€‚\")\n",
    "            reviews_df.rename(columns={review_column_name: 'review_text'}, inplace=True)\n",
    "            reviews_df.dropna(subset=['review_text'], inplace=True)\n",
    "            analyzer = SentimentIntensityAnalyzer()\n",
    "            reviews_df['sentiment'] = reviews_df['review_text'].apply(lambda text: analyzer.polarity_scores(str(text))['compound'])\n",
    "            if 'rating' not in reviews_df.columns:\n",
    "                reviews_df['rating'] = reviews_df['sentiment'].apply(sentiment_to_rating)\n",
    "            \n",
    "            print(\"\\næƒ…æ„Ÿåˆ†æžç»“æžœé¢„è§ˆ:\")\n",
    "            display(reviews_df.head())\n",
    "            print(\"\\né«˜åˆ†(>=4æ˜Ÿ)ä¸Žä½Žåˆ†(<=2æ˜Ÿ)è¯„è®ºå¯¹æ¯”:\")\n",
    "            display(reviews_df[reviews_df['rating'] >= 4].head(3))\n",
    "            display(reviews_df[reviews_df['rating'] <= 2].head(3))\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æƒ…æ„Ÿåˆ†æžå¤±è´¥: {e}\")\n",
    "\n",
    "    # ==================== 7. å¤šè¯­è¨€ç¿»è¯‘ ====================\n",
    "    if unesco_file:\n",
    "        print(\"\\n--- ðŸŒ æ­£åœ¨è¿›è¡Œéžé—æè¿°ç¿»è¯‘ (ä»…å‰5æ¡ä½œä¸ºæ¼”ç¤º) ---\")\n",
    "        try:\n",
    "            def translate_text(text, target_lang):\n",
    "                if not isinstance(text, str) or not text.strip(): return \"\"\n",
    "                try: return GoogleTranslator(source='auto', target=target_lang).translate(text)\n",
    "                except: return text\n",
    "            target_languages = ['de', 'fr', 'zh-cn']; unesco_translated_df = unesco_df.head(5).copy()\n",
    "            for lang in target_languages:\n",
    "                column_name = f'Description_{lang.upper()}'\n",
    "                unesco_translated_df[column_name] = unesco_translated_df['Description EN'].apply(lambda x: translate_text(x, lang))\n",
    "            print(\"\\nç¿»è¯‘å®ŒæˆåŽçš„æ•°æ®é¢„è§ˆï¼š\"); display(unesco_translated_df)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç¿»è¯‘å¤±è´¥: {e}\")\n",
    "            \n",
    "    print(\"\\n--- âœ¨ åˆ†æžå…¨éƒ¨å®Œæˆ ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9cbcd8-eb87-459e-a8f6-b0bf0019e78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ceb6bce81264acd9619ed64714da6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='é€‰æ‹©é”€å”®æ–‡ä»¶:', options=('Amazon Sale Report.csv', 'amazon-fashion-800k+-user-râ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2: åˆ›å»ºå¹¶æ˜¾ç¤ºäº¤äº’å¼ç•Œé¢\n",
    "# ==============================================================================\n",
    "import os\n",
    "from ipywidgets import interactive_output, Dropdown, VBox\n",
    "\n",
    "# æ‰«ææ–‡ä»¶å¹¶åˆ›å»ºä¸‹æ‹‰èœå•\n",
    "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "sales_report_options = [f for f in csv_files if 'amazon' in f.lower() or 'sales' in f.lower()]\n",
    "unesco_options = [f for f in csv_files if 'ich' in f.lower() or 'unesco' in f.lower()]\n",
    "reviews_options = [None] + [f for f in csv_files if 'review' in f.lower()]\n",
    "\n",
    "if not sales_report_options or not unesco_options:\n",
    "    print(\"é”™è¯¯ï¼šè¯·ç¡®ä¿é”€å”®æ–‡ä»¶å’ŒUNESCOæ–‡ä»¶éƒ½åœ¨å½“å‰ç›®å½•ä¸­ã€‚\")\n",
    "else:\n",
    "    # åˆ›å»ºæ‰€æœ‰æŽ§ä»¶\n",
    "    sales_dropdown = Dropdown(options=sales_report_options, description='é€‰æ‹©é”€å”®æ–‡ä»¶:')\n",
    "    unesco_dropdown = Dropdown(options=unesco_options, description='é€‰æ‹©UNESCOæ–‡ä»¶:')\n",
    "    reviews_dropdown = Dropdown(options=reviews_options, description='é€‰æ‹©è¯„è®ºæ–‡ä»¶ (å¯é€‰):')\n",
    "\n",
    "    # ä½¿ç”¨ interactive_output å°†æŽ§ä»¶ä¸Žå‡½æ•°è¾“å‡ºåˆ†ç¦»\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    def on_value_change(change):\n",
    "        with output_area:\n",
    "            run_analysis(sales_dropdown.value, unesco_dropdown.value, reviews_dropdown.value)\n",
    "\n",
    "    # ç›‘å¬æŽ§ä»¶å€¼çš„å˜åŒ–\n",
    "    sales_dropdown.observe(on_value_change, names='value')\n",
    "    unesco_dropdown.observe(on_value_change, names='value')\n",
    "    reviews_dropdown.observe(on_value_change, names='value')\n",
    "\n",
    "    # å°†æŽ§ä»¶å’Œè¾“å‡ºåŒºåŸŸä¸€èµ·æ˜¾ç¤ºå‡ºæ¥\n",
    "    display(VBox([sales_dropdown, unesco_dropdown, reviews_dropdown, output_area]))\n",
    "    \n",
    "    # é¦–æ¬¡æ‰‹åŠ¨è§¦å‘è¿è¡Œ\n",
    "    on_value_change(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd233d-df4c-4941-abd3-ea4d335c36e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
