{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e759b5-69ea-4e65-bd85-59ba6ba90e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 1: 导入所有库并进行环境设置 (已添加警告抑制)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 解决 KMeans 内存泄漏警告 (必须在导入 KMeans 之前设置)\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# 分析库\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "# (新增) NLP & 语义分析库\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 可视化库\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# NLP & 交互库\n",
    "from deep_translator import GoogleTranslator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# --- (新增) 抑制特定的Pandas警告，美化输出 ---\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning, DtypeWarning\n",
    "warnings.filterwarnings('ignore', category=SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=DtypeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00096841-adb5-40ee-9c0f-7e332d8444d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2: 定义核心辅助函数\n",
    "# ==============================================================================\n",
    "\n",
    "def sentiment_to_rating(sentiment):\n",
    "    \"\"\"这是一个标准的、多行情感评分转换函数\"\"\"\n",
    "    if sentiment >= 0.5:\n",
    "        return 5\n",
    "    elif sentiment >= 0.05:\n",
    "        return 4\n",
    "    elif sentiment > -0.05:\n",
    "        return 3\n",
    "    elif sentiment > -0.5:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def calculate_similarity(amazon_df, unesco_df):\n",
    "    \"\"\"语义相似度匹配核心函数\"\"\"\n",
    "    print(\"1. 准备文本语料库 (智能探测模式)...\")\n",
    "\n",
    "    # 'text_for_matching' 是在数据加载阶段智能创建的列\n",
    "    if 'text_for_matching' in amazon_df.columns and amazon_df['text_for_matching'].notna().any():\n",
    "        print(\"✅ 使用在数据加载阶段已准备好的 'text_for_matching' 列。\")\n",
    "    else:\n",
    "        # 如果 'text_for_matching' 列不可用，则执行备用探测\n",
    "        print(\"⚠️ 'text_for_matching' 列不可用，将执行备用探测...\")\n",
    "        title_candidates = ['title', 'product name', 'name', 'description', 'item_name', 'about_product']\n",
    "        category_candidates = ['category', 'product type', 'type']\n",
    "        \n",
    "        available_cols_lower = {col.lower(): col for col in amazon_df.columns}\n",
    "        cols_to_use = []\n",
    "        \n",
    "        for candidates in [title_candidates, category_candidates]:\n",
    "            for candidate in candidates:\n",
    "                if candidate in available_cols_lower:\n",
    "                    cols_to_use.append(available_cols_lower[candidate])\n",
    "                    break\n",
    "        \n",
    "        if cols_to_use:\n",
    "            print(f\"✅ 备用探测成功: {cols_to_use}。将合并其内容。\")\n",
    "            amazon_df['text_for_matching'] = amazon_df[cols_to_use].fillna('').astype(str).agg(' '.join, axis=1)\n",
    "        else:\n",
    "            print(\"⚠️ 备用探测失败。将回退至 'SKU'。\")\n",
    "            amazon_df['text_for_matching'] = amazon_df['SKU'].fillna('')\n",
    "        \n",
    "    unesco_df['text_for_matching'] = unesco_df['Description EN'].fillna('')\n",
    "    \n",
    "    corpus = pd.concat([amazon_df['text_for_matching'].fillna(''), unesco_df['text_for_matching'].fillna('')], ignore_index=True)\n",
    "\n",
    "    print(\"2. 正在进行 TF-IDF 文本向量化...\")\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    amazon_matrix = tfidf_matrix[:len(amazon_df)]\n",
    "    unesco_matrix = tfidf_matrix[len(amazon_df):]\n",
    "\n",
    "    print(\"3. 正在计算余弦相似度...\")\n",
    "    similarity_matrix = cosine_similarity(amazon_matrix, unesco_matrix)\n",
    "    \n",
    "    return similarity_matrix, vectorizer, list(unesco_df['Title EN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c6dab4-833f-44c3-81d4-7da66f055855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: 定义主分析流程函数 `run_analysis`\n",
    "# ==============================================================================\n",
    "\n",
    "def run_analysis(sales_file, unesco_file, reviews_file, metadata_file):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"--- 正在使用销售文件: '{sales_file}' ---\")\n",
    "    if metadata_file: print(f\"--- 正在使用元数据文件: '{metadata_file}' ---\")\n",
    "    if unesco_file: print(f\"--- 正在使用UNESCO文件: '{unesco_file}' ---\")\n",
    "    if reviews_file: print(f\"--- 正在使用评论文件: '{reviews_file}' ---\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # ==================== 1. 数据加载与清洗 (V4.3 - 终极健壮版) ====================\n",
    "    try:\n",
    "        print(\"--- 正在加载与清洗数据 ---\")\n",
    "        \n",
    "        amazon_df = pd.read_csv(sales_file, on_bad_lines='skip', dtype={'ASIN': str, 'asin': str})\n",
    "        if unesco_file:\n",
    "            unesco_df = pd.read_csv(unesco_file, encoding=\"utf-8-sig\", on_bad_lines='skip')\n",
    "\n",
    "        if metadata_file:\n",
    "            print(f\"--- 发现元数据文件 '{metadata_file}'，准备进行动态合并 ---\")\n",
    "            metadata_df = pd.read_csv(metadata_file, on_bad_lines='skip', dtype={'ASIN': str, 'asin': str})\n",
    "            \n",
    "            sales_key_col = next((col for col in ['ASIN', 'asin'] if col in amazon_df.columns), None)\n",
    "            metadata_key_col = next((col for col in ['asin', 'ASIN'] if col in metadata_df.columns), None)\n",
    "            \n",
    "            desc_candidates = ['about_product', 'description', 'title', 'product_name', 'name', 'item_name']\n",
    "            metadata_desc_col = next((col for col in desc_candidates if col in metadata_df.columns), None)\n",
    "            \n",
    "            if sales_key_col and metadata_key_col and metadata_desc_col:\n",
    "                print(f\"✅ 自动探测成功: 将使用 '{sales_key_col}' 和 '{metadata_key_col}' 作为共同键。\")\n",
    "                print(f\"✅ 将使用 '{metadata_desc_col}' 作为商品描述来源。\")\n",
    "\n",
    "                print(\"--> 正在对ASIN键进行标准化清洗 (转为文本并移除前后空格)...\")\n",
    "                amazon_df[sales_key_col] = amazon_df[sales_key_col].astype(str).str.strip()\n",
    "                metadata_df[metadata_key_col] = metadata_df[metadata_key_col].astype(str).str.strip()\n",
    "\n",
    "                metadata_subset = metadata_df[[metadata_key_col, metadata_desc_col]].drop_duplicates(subset=[metadata_key_col])\n",
    "                amazon_df = pd.merge(amazon_df, metadata_subset, left_on=sales_key_col, right_on=metadata_key_col, how='left')\n",
    "                \n",
    "                total_rows = len(amazon_df)\n",
    "                matched_rows = amazon_df[metadata_desc_col].notna().sum()\n",
    "                match_rate = (matched_rows / total_rows) * 100 if total_rows > 0 else 0\n",
    "                print(f\"📊 数据合并完成！匹配成功率: {match_rate:.2f}% ({matched_rows} / {total_rows} 条记录)。\")\n",
    "\n",
    "                fallback_text = amazon_df['Category'].fillna('') + ' ' + amazon_df.get('Style', pd.Series(index=amazon_df.index, dtype=str)).fillna('')\n",
    "                amazon_df['text_for_matching'] = amazon_df[metadata_desc_col].fillna(fallback_text)\n",
    "                print(\"--> 已为所有商品创建最终描述文本 'text_for_matching'。\")\n",
    "            else:\n",
    "                print(\"⚠️ 警告: 无法完成合并。原因如下:\")\n",
    "                if not sales_key_col: print(\"  - 主销售文件中未找到ASIN键。\")\n",
    "                if not metadata_key_col: print(\"  - 元数据文件中未找到ASIN键。\")\n",
    "                if not metadata_desc_col: print(\"  - 元数据文件中未找到可用的描述列。\")\n",
    "                amazon_df['text_for_matching'] = amazon_df['Category'].fillna('')\n",
    "        else:\n",
    "            print(\"--- 未选择元数据文件，跳过数据合并 ---\")\n",
    "            amazon_df['text_for_matching'] = amazon_df['Category'].fillna('') + ' ' + amazon_df.get('Style', pd.Series(index=amazon_df.index, dtype=str)).fillna('')\n",
    "        \n",
    "        if 'Total Sales' in amazon_df.columns: amazon_df.rename(columns={'Total Sales': 'Amount'}, inplace=True)\n",
    "        if 'Product' in amazon_df.columns: amazon_df.rename(columns={'Product': 'SKU'}, inplace=True)\n",
    "        if 'Qty' not in amazon_df.columns and 'Quantity' in amazon_df.columns: amazon_df.rename(columns={'Quantity': 'Qty'}, inplace=True)\n",
    "        if 'Order ID' not in amazon_df.columns and 'Order_ID' in amazon_df.columns: amazon_df.rename(columns={'Order_ID': 'Order ID'}, inplace=True)\n",
    "        \n",
    "        required_cols = [\"Amount\", \"Category\", \"Date\", \"Status\", \"SKU\", \"Order ID\", \"Qty\"]\n",
    "        if any(col not in amazon_df.columns for col in required_cols): raise ValueError(f\"文件 '{sales_file}' 缺少必需的列。\")\n",
    "        \n",
    "        amazon_df.dropna(subset=[\"Amount\", \"Category\", \"Date\"], inplace=True)\n",
    "        try:\n",
    "            amazon_df[\"Date\"] = pd.to_datetime(amazon_df[\"Date\"], format='%m-%d-%y')\n",
    "        except ValueError:\n",
    "            amazon_df[\"Date\"] = pd.to_datetime(amazon_df[\"Date\"], errors='coerce')\n",
    "            \n",
    "        amazon_df[\"Amount\"] = pd.to_numeric(amazon_df[\"Amount\"], errors='coerce')\n",
    "        valid_statuses = [\"Shipped\", \"Shipped - Delivered to Buyer\", \"Completed\", \"Pending\", \"Cancelled\"]\n",
    "        amazon_df = amazon_df[amazon_df[\"Status\"].isin(valid_statuses)]\n",
    "        amazon_df.dropna(subset=['Date', 'Amount', 'SKU', 'Order ID', 'Qty'], inplace=True)\n",
    "        \n",
    "        all_categories = amazon_df['Category'].unique()\n",
    "        non遗_products = amazon_df[amazon_df['Category'].str.contains('|'.join(all_categories), case=False, na=False)]\n",
    "        \n",
    "        print(\"✅ 数据加载和清洗完成！\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 处理数据时出错: {e}\"); return\n",
    "\n",
    "    # ==================== 2. 过滤与映射 ====================\n",
    "    print(\"\\n--- 正在进行过滤与映射 ---\")\n",
    "    relevant_unesco = pd.DataFrame()\n",
    "    if unesco_file and 'unesco_df' in locals():\n",
    "        keywords = ['craft', 'textile', 'embroidery', 'weaving', 'costume', 'dress', 'heritage product', 'handicraft']\n",
    "        relevant_unesco = unesco_df[unesco_df['Description EN'].str.contains('|'.join(keywords), case=False, na=False)]\n",
    "        print(f\"根据关键词筛选后，找到 {len(relevant_unesco)} 个相关的非遗项目。\")\n",
    "    print(f\"潜在非遗产品订单: {len(non遗_products)}\\n\")\n",
    "\n",
    "    # ==================== 3. 时间序列预测 (LSTM) ====================\n",
    "    print(\"\\n--- 🧠 正在进行 LSTM 深度学习预测 ---\")\n",
    "    try:\n",
    "        sales_ts = amazon_df.groupby('Date')['Amount'].sum().asfreq('D', fill_value=0)\n",
    "        sales_values = sales_ts.values.reshape(-1, 1)\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_values = scaler.fit_transform(sales_values)\n",
    "\n",
    "        def create_dataset(data, look_back=7):\n",
    "            X, y = [], []\n",
    "            for i in range(len(data) - look_back):\n",
    "                X.append(data[i:(i + look_back), 0])\n",
    "                y.append(data[i + look_back, 0])\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "        look_back = 7\n",
    "        X, y = create_dataset(scaled_values, look_back)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "        model = Sequential([Input(shape=(look_back, 1)), LSTM(50), Dense(1)])\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        print(\"正在训练模型...\")\n",
    "        model.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
    "        print(\"正在预测未来...\")\n",
    "        last_days_scaled = scaled_values[-look_back:]\n",
    "        current_input = np.reshape(last_days_scaled, (1, look_back, 1))\n",
    "        future_predictions_scaled = []\n",
    "        for _ in range(30):\n",
    "            next_pred_scaled = model.predict(current_input, verbose=0)\n",
    "            future_predictions_scaled.append(next_pred_scaled[0, 0])\n",
    "            new_pred_reshaped = np.reshape(next_pred_scaled, (1, 1, 1))\n",
    "            current_input = np.append(current_input[:, 1:, :], new_pred_reshaped, axis=1)\n",
    "        future_predictions = scaler.inverse_transform(np.array(future_predictions_scaled).reshape(-1, 1))\n",
    "        last_date = sales_ts.index[-1]\n",
    "        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30)\n",
    "        fig_lstm = go.Figure()\n",
    "        fig_lstm.add_trace(go.Scatter(x=sales_ts.index, y=sales_ts.values, name='历史销售额', line=dict(color='royalblue', width=2), fill='tozeroy', fillcolor='rgba(65, 105, 225, 0.2)'))\n",
    "        fig_lstm.add_trace(go.Scatter(x=future_dates, y=future_predictions.flatten(), name='LSTM 预测销售额', line=dict(color='darkorange', dash='dash', width=2), fill='tozeroy', fillcolor='rgba(255, 140, 0, 0.2)'))\n",
    "        fig_lstm.update_layout(title='未来30天销售额深度学习预测 (LSTM模型)')\n",
    "        fig_lstm.show()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ LSTM 预测失败: {e}\")\n",
    "\n",
    "    # ==================== 4. 类别销售可视化 ====================\n",
    "    print(\"\\n--- 🛍️ 正在生成品类表现图 ---\")\n",
    "    category_means = non遗_products.groupby('Category')['Amount'].mean().sort_values(ascending=False).reset_index()\n",
    "    fig_bar = px.bar(category_means, x='Category', y='Amount', color='Category', text_auto='.2f', title='各产品类别平均销售额对比')\n",
    "    fig_bar.update_layout(width=800, height=500, showlegend=False); fig_bar.show()\n",
    "\n",
    "    # ==================== 5. 商品聚类分析 ====================\n",
    "    print(\"\\n--- 🔥 正在进行商品聚类分析 ---\")\n",
    "    try:\n",
    "        product_agg_df = amazon_df.groupby('SKU').agg(total_amount=('Amount', 'sum'), total_qty=('Qty', 'sum'), order_count=('Order ID', 'nunique')).reset_index()\n",
    "        features_to_cluster = ['total_amount', 'total_qty', 'order_count']; features = product_agg_df[features_to_cluster]\n",
    "        scaler = StandardScaler(); features_scaled = scaler.fit_transform(features)\n",
    "        kmeans = KMeans(n_clusters=3, n_init=10, random_state=42); product_agg_df.loc[:, 'cluster'] = kmeans.fit_predict(features_scaled)\n",
    "        cluster_summary = product_agg_df.groupby('cluster')[features_to_cluster].mean().sort_values(by='total_amount', ascending=False)\n",
    "        hot_product_cluster_id = cluster_summary.index[0]\n",
    "        hot_products = product_agg_df[product_agg_df['cluster'] == hot_product_cluster_id].sort_values(by='total_amount', ascending=False)\n",
    "        print(\"\\n每个商品簇的特征均值:\"); display(cluster_summary)\n",
    "        print(\"\\n排名前10的热销商品:\"); display(hot_products.head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 商品聚类失败: {e}\")\n",
    "\n",
    "    # ==================== 6. 情感分析 ====================\n",
    "    if reviews_file:\n",
    "        print(\"\\n--- 💬 正在进行情感分析 ---\")\n",
    "        try:\n",
    "            def find_review_column(df):\n",
    "                priority_cols = ['reviews.text', 'review_text', 'content', 'comment', 'review']\n",
    "                for p_col in priority_cols:\n",
    "                    if p_col in df.columns and df[p_col].dropna().astype(str).str.strip().any(): return p_col\n",
    "                object_cols = df.select_dtypes(include=['object']).columns\n",
    "                if not object_cols.empty:\n",
    "                    return max(object_cols, key=lambda col: df[col].dropna().astype(str).str.len().mean())\n",
    "                return None\n",
    "            reviews_df = pd.read_csv(reviews_file)\n",
    "            review_column_name = find_review_column(reviews_df)\n",
    "            if review_column_name is None: raise ValueError(\"未能自动检测到文本列。\")\n",
    "            reviews_df.rename(columns={review_column_name: 'review_text'}, inplace=True)\n",
    "            reviews_df.dropna(subset=['review_text'], inplace=True)\n",
    "            analyzer = SentimentIntensityAnalyzer()\n",
    "            reviews_df['sentiment'] = reviews_df['review_text'].apply(lambda text: analyzer.polarity_scores(str(text))['compound'])\n",
    "            if 'rating' not in reviews_df.columns:\n",
    "                reviews_df['rating'] = reviews_df['sentiment'].apply(sentiment_to_rating)\n",
    "            print(\"\\n情感分析结果预览:\"); display(reviews_df.head())\n",
    "            print(\"\\n高分(>=4星)与低分(<=2星)评论对比:\"); display(reviews_df[reviews_df['rating'] >= 4].head(3)); display(reviews_df[reviews_df['rating'] <= 2].head(3))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 情感分析失败: {e}\")\n",
    "\n",
    "    # ==================== 7. 多语言翻译 ====================\n",
    "    if unesco_file and 'unesco_df' in locals():\n",
    "        print(\"\\n--- 🌍 正在进行非遗描述翻译 (仅前5条作为演示) ---\")\n",
    "        try:\n",
    "            def translate_text(text, target_lang):\n",
    "                if not isinstance(text, str) or not text.strip(): return \"\"\n",
    "                try: return GoogleTranslator(source='auto', target=target_lang).translate(text)\n",
    "                except: return text\n",
    "            target_languages = ['de', 'fr', 'zh-cn']; unesco_translated_df = unesco_df.head(5).copy()\n",
    "            for lang in target_languages:\n",
    "                column_name = f'Description_{lang.upper()}'\n",
    "                unesco_translated_df[column_name] = unesco_translated_df['Description EN'].apply(lambda x: translate_text(x, lang))\n",
    "            print(\"\\n翻译完成后的数据预览：\"); display(unesco_translated_df)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 翻译失败: {e}\")\n",
    "            \n",
    "    # ==================== 8. (新增) 语义相似度匹配 ====================\n",
    "    if unesco_file and not relevant_unesco.empty:\n",
    "        print(\"\\n--- 🔗 正在进行语义相似度匹配 ---\")\n",
    "        try:\n",
    "            cosine_sim, tfidf_vectorizer, unesco_titles = calculate_similarity(amazon_df, relevant_unesco)\n",
    "            print(\"✅ 相似度计算完成！\")\n",
    "            print(f\"--> 生成的相似度矩阵形状: {cosine_sim.shape} (商品数, 非遗项目数)\")\n",
    "            print(\"\\n--- 场景1: 为热销商品匹配文化灵感 ---\")\n",
    "            if 'hot_products' in locals() and not hot_products.empty:\n",
    "                product_sku_to_test = hot_products.iloc[0]['SKU']\n",
    "                product_indices = amazon_df.index[amazon_df['SKU'] == product_sku_to_test].tolist()\n",
    "                if product_indices:\n",
    "                    product_idx = product_indices[0]\n",
    "                    sim_scores = list(enumerate(cosine_sim[product_idx]))\n",
    "                    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "                    top_5_indices = [i[0] for i in sim_scores[0:5]]\n",
    "                    print(f\"与最热销商品 '{product_sku_to_test}' 最相似的前5个非遗项目是:\")\n",
    "                    for idx in top_5_indices:\n",
    "                        print(f\"- {unesco_titles[idx]} (相似度: {cosine_sim[product_idx, idx]:.4f})\")\n",
    "                else:\n",
    "                    print(f\"未能在大表中定位到热销商品 '{product_sku_to_test}'，跳过场景1。\")\n",
    "            else:\n",
    "                print(\"未找到热销商品列表 (可能聚类失败)，跳过场景1。\")\n",
    "\n",
    "            print(\"\\n--- 场景2: 根据文化元素反向寻找潜力商品 ---\")\n",
    "            if unesco_titles:\n",
    "                heritage_idx = np.random.randint(0, len(unesco_titles))\n",
    "                heritage_title_to_test = unesco_titles[heritage_idx]\n",
    "                sim_scores_for_heritage = list(enumerate(cosine_sim[:, heritage_idx]))\n",
    "                sim_scores_for_heritage = sorted(sim_scores_for_heritage, key=lambda x: x[1], reverse=True)\n",
    "                top_10_product_indices = [i[0] for i in sim_scores_for_heritage[0:10]]\n",
    "                print(f\"\\n与非遗项目 '{heritage_title_to_test}' 最相似的前10个在售商品是:\")\n",
    "                recommended_products = amazon_df.iloc[top_10_product_indices]\n",
    "                display(recommended_products[['SKU', 'Amount', 'Category']])\n",
    "            else:\n",
    "                print(\"无可用的非遗项目标题，跳过场景2。\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 语义相似度匹配失败: {e}\")\n",
    "    else:\n",
    "        print(\"\\n--- 🔗 跳过语义相似度匹配：未提供UNESCO文件或未找到相关的非遗项目 ---\")\n",
    "            \n",
    "    print(\"\\n--- ✨ 分析全部完成 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03325421-bc8d-4fcf-a38e-fa46d0e43c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8d4ecc4114499caba00631faad8627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='销售文件:', options=('data\\\\Amazon Sale Report.csv', 'data\\\\amazon-fashion-80…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: 创建并显示交互式界面\n",
    "# ==============================================================================\n",
    "import os\n",
    "from ipywidgets import interactive_output, Dropdown, VBox\n",
    "\n",
    "data_folder = 'data'\n",
    "\n",
    "if not os.path.isdir(data_folder):\n",
    "    print(f\"错误：请先创建名为 '{data_folder}' 的文件夹，并将所有数据文件放入其中。\")\n",
    "else:\n",
    "    all_data_files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) \n",
    "                      if f.endswith(('.csv', '.xls', 'xlsx'))]\n",
    "\n",
    "    sales_report_options = [f for f in all_data_files if 'amazon' in f.lower() or 'sales' in f.lower()]\n",
    "    unesco_options = [f for f in all_data_files if 'ich' in f.lower() or 'unesco' in f.lower()]\n",
    "    reviews_options = [None] + [f for f in all_data_files if 'review' in f.lower()]\n",
    "    metadata_options = [None] + all_data_files\n",
    "\n",
    "    if not sales_report_options or not unesco_options:\n",
    "        print(f\"错误：请确保销售文件和UNESCO文件都在 '{data_folder}' 目录中。\")\n",
    "    else:\n",
    "        # 创建所有控件\n",
    "        sales_dropdown = Dropdown(options=sales_report_options, description='销售文件:')\n",
    "        metadata_dropdown = Dropdown(options=metadata_options, description='元数据文件 (可选):')\n",
    "        unesco_dropdown = Dropdown(options=unesco_options, description='UNESCO文件:')\n",
    "        reviews_dropdown = Dropdown(options=reviews_options, description='评论文件 (可选):')\n",
    "\n",
    "        output_area = widgets.Output()\n",
    "\n",
    "        def on_value_change(change):\n",
    "            with output_area:\n",
    "                run_analysis(sales_dropdown.value, unesco_dropdown.value, reviews_dropdown.value, metadata_dropdown.value)\n",
    "\n",
    "        # 监听所有控件的变化\n",
    "        sales_dropdown.observe(on_value_change, names='value')\n",
    "        metadata_dropdown.observe(on_value_change, names='value')\n",
    "        unesco_dropdown.observe(on_value_change, names='value')\n",
    "        reviews_dropdown.observe(on_value_change, names='value')\n",
    "\n",
    "        # 将所有控件和输出区域一起显示出来\n",
    "        display(VBox([sales_dropdown, metadata_dropdown, unesco_dropdown, reviews_dropdown, output_area]))\n",
    "        \n",
    "        # 首次手动触发运行\n",
    "        on_value_change(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df3b77-595b-4542-811f-37903e69992c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
