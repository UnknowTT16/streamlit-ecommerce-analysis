{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e759b5-69ea-4e65-bd85-59ba6ba90e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 1: å¯¼å…¥æ‰€æœ‰åº“å¹¶è¿›è¡Œç¯å¢ƒè®¾ç½® (å·²æ·»åŠ è­¦å‘ŠæŠ‘åˆ¶)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# è§£å†³ KMeans å†…å­˜æ³„æ¼è­¦å‘Š (å¿…é¡»åœ¨å¯¼å…¥ KMeans ä¹‹å‰è®¾ç½®)\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# åˆ†æåº“\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "# (æ–°å¢) NLP & è¯­ä¹‰åˆ†æåº“\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# å¯è§†åŒ–åº“\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# NLP & äº¤äº’åº“\n",
    "from deep_translator import GoogleTranslator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# --- (æ–°å¢) æŠ‘åˆ¶ç‰¹å®šçš„Pandasè­¦å‘Šï¼Œç¾åŒ–è¾“å‡º ---\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning, DtypeWarning\n",
    "warnings.filterwarnings('ignore', category=SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=DtypeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00096841-adb5-40ee-9c0f-7e332d8444d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2: å®šä¹‰æ ¸å¿ƒè¾…åŠ©å‡½æ•°\n",
    "# ==============================================================================\n",
    "\n",
    "def sentiment_to_rating(sentiment):\n",
    "    \"\"\"è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„ã€å¤šè¡Œæƒ…æ„Ÿè¯„åˆ†è½¬æ¢å‡½æ•°\"\"\"\n",
    "    if sentiment >= 0.5:\n",
    "        return 5\n",
    "    elif sentiment >= 0.05:\n",
    "        return 4\n",
    "    elif sentiment > -0.05:\n",
    "        return 3\n",
    "    elif sentiment > -0.5:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def calculate_similarity(amazon_df, unesco_df):\n",
    "    \"\"\"è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…æ ¸å¿ƒå‡½æ•°\"\"\"\n",
    "    print(\"1. å‡†å¤‡æ–‡æœ¬è¯­æ–™åº“ (æ™ºèƒ½æ¢æµ‹æ¨¡å¼)...\")\n",
    "\n",
    "    # 'text_for_matching' æ˜¯åœ¨æ•°æ®åŠ è½½é˜¶æ®µæ™ºèƒ½åˆ›å»ºçš„åˆ—\n",
    "    if 'text_for_matching' in amazon_df.columns and amazon_df['text_for_matching'].notna().any():\n",
    "        print(\"âœ… ä½¿ç”¨åœ¨æ•°æ®åŠ è½½é˜¶æ®µå·²å‡†å¤‡å¥½çš„ 'text_for_matching' åˆ—ã€‚\")\n",
    "    else:\n",
    "        # å¦‚æœ 'text_for_matching' åˆ—ä¸å¯ç”¨ï¼Œåˆ™æ‰§è¡Œå¤‡ç”¨æ¢æµ‹\n",
    "        print(\"âš ï¸ 'text_for_matching' åˆ—ä¸å¯ç”¨ï¼Œå°†æ‰§è¡Œå¤‡ç”¨æ¢æµ‹...\")\n",
    "        title_candidates = ['title', 'product name', 'name', 'description', 'item_name', 'about_product']\n",
    "        category_candidates = ['category', 'product type', 'type']\n",
    "        \n",
    "        available_cols_lower = {col.lower(): col for col in amazon_df.columns}\n",
    "        cols_to_use = []\n",
    "        \n",
    "        for candidates in [title_candidates, category_candidates]:\n",
    "            for candidate in candidates:\n",
    "                if candidate in available_cols_lower:\n",
    "                    cols_to_use.append(available_cols_lower[candidate])\n",
    "                    break\n",
    "        \n",
    "        if cols_to_use:\n",
    "            print(f\"âœ… å¤‡ç”¨æ¢æµ‹æˆåŠŸ: {cols_to_use}ã€‚å°†åˆå¹¶å…¶å†…å®¹ã€‚\")\n",
    "            amazon_df['text_for_matching'] = amazon_df[cols_to_use].fillna('').astype(str).agg(' '.join, axis=1)\n",
    "        else:\n",
    "            print(\"âš ï¸ å¤‡ç”¨æ¢æµ‹å¤±è´¥ã€‚å°†å›é€€è‡³ 'SKU'ã€‚\")\n",
    "            amazon_df['text_for_matching'] = amazon_df['SKU'].fillna('')\n",
    "        \n",
    "    unesco_df['text_for_matching'] = unesco_df['Description EN'].fillna('')\n",
    "    \n",
    "    corpus = pd.concat([amazon_df['text_for_matching'].fillna(''), unesco_df['text_for_matching'].fillna('')], ignore_index=True)\n",
    "\n",
    "    print(\"2. æ­£åœ¨è¿›è¡Œ TF-IDF æ–‡æœ¬å‘é‡åŒ–...\")\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    amazon_matrix = tfidf_matrix[:len(amazon_df)]\n",
    "    unesco_matrix = tfidf_matrix[len(amazon_df):]\n",
    "\n",
    "    print(\"3. æ­£åœ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦...\")\n",
    "    similarity_matrix = cosine_similarity(amazon_matrix, unesco_matrix)\n",
    "    \n",
    "    return similarity_matrix, vectorizer, list(unesco_df['Title EN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c6dab4-833f-44c3-81d4-7da66f055855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: å®šä¹‰ä¸»åˆ†ææµç¨‹å‡½æ•° `run_analysis`\n",
    "# ==============================================================================\n",
    "\n",
    "def run_analysis(sales_file, unesco_file, reviews_file, metadata_file):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"--- æ­£åœ¨ä½¿ç”¨é”€å”®æ–‡ä»¶: '{sales_file}' ---\")\n",
    "    if metadata_file: print(f\"--- æ­£åœ¨ä½¿ç”¨å…ƒæ•°æ®æ–‡ä»¶: '{metadata_file}' ---\")\n",
    "    if unesco_file: print(f\"--- æ­£åœ¨ä½¿ç”¨UNESCOæ–‡ä»¶: '{unesco_file}' ---\")\n",
    "    if reviews_file: print(f\"--- æ­£åœ¨ä½¿ç”¨è¯„è®ºæ–‡ä»¶: '{reviews_file}' ---\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # ==================== 1. æ•°æ®åŠ è½½ä¸æ¸…æ´— (V4.3 - ç»ˆæå¥å£®ç‰ˆ) ====================\n",
    "    try:\n",
    "        print(\"--- æ­£åœ¨åŠ è½½ä¸æ¸…æ´—æ•°æ® ---\")\n",
    "        \n",
    "        amazon_df = pd.read_csv(sales_file, on_bad_lines='skip', dtype={'ASIN': str, 'asin': str})\n",
    "        if unesco_file:\n",
    "            unesco_df = pd.read_csv(unesco_file, encoding=\"utf-8-sig\", on_bad_lines='skip')\n",
    "\n",
    "        if metadata_file:\n",
    "            print(f\"--- å‘ç°å…ƒæ•°æ®æ–‡ä»¶ '{metadata_file}'ï¼Œå‡†å¤‡è¿›è¡ŒåŠ¨æ€åˆå¹¶ ---\")\n",
    "            metadata_df = pd.read_csv(metadata_file, on_bad_lines='skip', dtype={'ASIN': str, 'asin': str})\n",
    "            \n",
    "            sales_key_col = next((col for col in ['ASIN', 'asin'] if col in amazon_df.columns), None)\n",
    "            metadata_key_col = next((col for col in ['asin', 'ASIN'] if col in metadata_df.columns), None)\n",
    "            \n",
    "            desc_candidates = ['about_product', 'description', 'title', 'product_name', 'name', 'item_name']\n",
    "            metadata_desc_col = next((col for col in desc_candidates if col in metadata_df.columns), None)\n",
    "            \n",
    "            if sales_key_col and metadata_key_col and metadata_desc_col:\n",
    "                print(f\"âœ… è‡ªåŠ¨æ¢æµ‹æˆåŠŸ: å°†ä½¿ç”¨ '{sales_key_col}' å’Œ '{metadata_key_col}' ä½œä¸ºå…±åŒé”®ã€‚\")\n",
    "                print(f\"âœ… å°†ä½¿ç”¨ '{metadata_desc_col}' ä½œä¸ºå•†å“æè¿°æ¥æºã€‚\")\n",
    "\n",
    "                print(\"--> æ­£åœ¨å¯¹ASINé”®è¿›è¡Œæ ‡å‡†åŒ–æ¸…æ´— (è½¬ä¸ºæ–‡æœ¬å¹¶ç§»é™¤å‰åç©ºæ ¼)...\")\n",
    "                amazon_df[sales_key_col] = amazon_df[sales_key_col].astype(str).str.strip()\n",
    "                metadata_df[metadata_key_col] = metadata_df[metadata_key_col].astype(str).str.strip()\n",
    "\n",
    "                metadata_subset = metadata_df[[metadata_key_col, metadata_desc_col]].drop_duplicates(subset=[metadata_key_col])\n",
    "                amazon_df = pd.merge(amazon_df, metadata_subset, left_on=sales_key_col, right_on=metadata_key_col, how='left')\n",
    "                \n",
    "                total_rows = len(amazon_df)\n",
    "                matched_rows = amazon_df[metadata_desc_col].notna().sum()\n",
    "                match_rate = (matched_rows / total_rows) * 100 if total_rows > 0 else 0\n",
    "                print(f\"ğŸ“Š æ•°æ®åˆå¹¶å®Œæˆï¼åŒ¹é…æˆåŠŸç‡: {match_rate:.2f}% ({matched_rows} / {total_rows} æ¡è®°å½•)ã€‚\")\n",
    "\n",
    "                fallback_text = amazon_df['Category'].fillna('') + ' ' + amazon_df.get('Style', pd.Series(index=amazon_df.index, dtype=str)).fillna('')\n",
    "                amazon_df['text_for_matching'] = amazon_df[metadata_desc_col].fillna(fallback_text)\n",
    "                print(\"--> å·²ä¸ºæ‰€æœ‰å•†å“åˆ›å»ºæœ€ç»ˆæè¿°æ–‡æœ¬ 'text_for_matching'ã€‚\")\n",
    "            else:\n",
    "                print(\"âš ï¸ è­¦å‘Š: æ— æ³•å®Œæˆåˆå¹¶ã€‚åŸå› å¦‚ä¸‹:\")\n",
    "                if not sales_key_col: print(\"  - ä¸»é”€å”®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ASINé”®ã€‚\")\n",
    "                if not metadata_key_col: print(\"  - å…ƒæ•°æ®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ASINé”®ã€‚\")\n",
    "                if not metadata_desc_col: print(\"  - å…ƒæ•°æ®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å¯ç”¨çš„æè¿°åˆ—ã€‚\")\n",
    "                amazon_df['text_for_matching'] = amazon_df['Category'].fillna('')\n",
    "        else:\n",
    "            print(\"--- æœªé€‰æ‹©å…ƒæ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡æ•°æ®åˆå¹¶ ---\")\n",
    "            amazon_df['text_for_matching'] = amazon_df['Category'].fillna('') + ' ' + amazon_df.get('Style', pd.Series(index=amazon_df.index, dtype=str)).fillna('')\n",
    "        \n",
    "        if 'Total Sales' in amazon_df.columns: amazon_df.rename(columns={'Total Sales': 'Amount'}, inplace=True)\n",
    "        if 'Product' in amazon_df.columns: amazon_df.rename(columns={'Product': 'SKU'}, inplace=True)\n",
    "        if 'Qty' not in amazon_df.columns and 'Quantity' in amazon_df.columns: amazon_df.rename(columns={'Quantity': 'Qty'}, inplace=True)\n",
    "        if 'Order ID' not in amazon_df.columns and 'Order_ID' in amazon_df.columns: amazon_df.rename(columns={'Order_ID': 'Order ID'}, inplace=True)\n",
    "        \n",
    "        required_cols = [\"Amount\", \"Category\", \"Date\", \"Status\", \"SKU\", \"Order ID\", \"Qty\"]\n",
    "        if any(col not in amazon_df.columns for col in required_cols): raise ValueError(f\"æ–‡ä»¶ '{sales_file}' ç¼ºå°‘å¿…éœ€çš„åˆ—ã€‚\")\n",
    "        \n",
    "        amazon_df.dropna(subset=[\"Amount\", \"Category\", \"Date\"], inplace=True)\n",
    "        try:\n",
    "            amazon_df[\"Date\"] = pd.to_datetime(amazon_df[\"Date\"], format='%m-%d-%y')\n",
    "        except ValueError:\n",
    "            amazon_df[\"Date\"] = pd.to_datetime(amazon_df[\"Date\"], errors='coerce')\n",
    "            \n",
    "        amazon_df[\"Amount\"] = pd.to_numeric(amazon_df[\"Amount\"], errors='coerce')\n",
    "        valid_statuses = [\"Shipped\", \"Shipped - Delivered to Buyer\", \"Completed\", \"Pending\", \"Cancelled\"]\n",
    "        amazon_df = amazon_df[amazon_df[\"Status\"].isin(valid_statuses)]\n",
    "        amazon_df.dropna(subset=['Date', 'Amount', 'SKU', 'Order ID', 'Qty'], inplace=True)\n",
    "        \n",
    "        all_categories = amazon_df['Category'].unique()\n",
    "        noné—_products = amazon_df[amazon_df['Category'].str.contains('|'.join(all_categories), case=False, na=False)]\n",
    "        \n",
    "        print(\"âœ… æ•°æ®åŠ è½½å’Œæ¸…æ´—å®Œæˆï¼\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}\"); return\n",
    "\n",
    "    # ==================== 2. è¿‡æ»¤ä¸æ˜ å°„ ====================\n",
    "    print(\"\\n--- æ­£åœ¨è¿›è¡Œè¿‡æ»¤ä¸æ˜ å°„ ---\")\n",
    "    relevant_unesco = pd.DataFrame()\n",
    "    if unesco_file and 'unesco_df' in locals():\n",
    "        keywords = ['craft', 'textile', 'embroidery', 'weaving', 'costume', 'dress', 'heritage product', 'handicraft']\n",
    "        relevant_unesco = unesco_df[unesco_df['Description EN'].str.contains('|'.join(keywords), case=False, na=False)]\n",
    "        print(f\"æ ¹æ®å…³é”®è¯ç­›é€‰åï¼Œæ‰¾åˆ° {len(relevant_unesco)} ä¸ªç›¸å…³çš„éé—é¡¹ç›®ã€‚\")\n",
    "    print(f\"æ½œåœ¨éé—äº§å“è®¢å•: {len(noné—_products)}\\n\")\n",
    "\n",
    "    # ==================== 3. æ—¶é—´åºåˆ—é¢„æµ‹ (LSTM) ====================\n",
    "    print(\"\\n--- ğŸ§  æ­£åœ¨è¿›è¡Œ LSTM æ·±åº¦å­¦ä¹ é¢„æµ‹ ---\")\n",
    "    try:\n",
    "        sales_ts = amazon_df.groupby('Date')['Amount'].sum().asfreq('D', fill_value=0)\n",
    "        sales_values = sales_ts.values.reshape(-1, 1)\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_values = scaler.fit_transform(sales_values)\n",
    "\n",
    "        def create_dataset(data, look_back=7):\n",
    "            X, y = [], []\n",
    "            for i in range(len(data) - look_back):\n",
    "                X.append(data[i:(i + look_back), 0])\n",
    "                y.append(data[i + look_back, 0])\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "        look_back = 7\n",
    "        X, y = create_dataset(scaled_values, look_back)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "        model = Sequential([Input(shape=(look_back, 1)), LSTM(50), Dense(1)])\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        print(\"æ­£åœ¨è®­ç»ƒæ¨¡å‹...\")\n",
    "        model.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
    "        print(\"æ­£åœ¨é¢„æµ‹æœªæ¥...\")\n",
    "        last_days_scaled = scaled_values[-look_back:]\n",
    "        current_input = np.reshape(last_days_scaled, (1, look_back, 1))\n",
    "        future_predictions_scaled = []\n",
    "        for _ in range(30):\n",
    "            next_pred_scaled = model.predict(current_input, verbose=0)\n",
    "            future_predictions_scaled.append(next_pred_scaled[0, 0])\n",
    "            new_pred_reshaped = np.reshape(next_pred_scaled, (1, 1, 1))\n",
    "            current_input = np.append(current_input[:, 1:, :], new_pred_reshaped, axis=1)\n",
    "        future_predictions = scaler.inverse_transform(np.array(future_predictions_scaled).reshape(-1, 1))\n",
    "        last_date = sales_ts.index[-1]\n",
    "        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30)\n",
    "        fig_lstm = go.Figure()\n",
    "        fig_lstm.add_trace(go.Scatter(x=sales_ts.index, y=sales_ts.values, name='å†å²é”€å”®é¢', line=dict(color='royalblue', width=2), fill='tozeroy', fillcolor='rgba(65, 105, 225, 0.2)'))\n",
    "        fig_lstm.add_trace(go.Scatter(x=future_dates, y=future_predictions.flatten(), name='LSTM é¢„æµ‹é”€å”®é¢', line=dict(color='darkorange', dash='dash', width=2), fill='tozeroy', fillcolor='rgba(255, 140, 0, 0.2)'))\n",
    "        fig_lstm.update_layout(title='æœªæ¥30å¤©é”€å”®é¢æ·±åº¦å­¦ä¹ é¢„æµ‹ (LSTMæ¨¡å‹)')\n",
    "        fig_lstm.show()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LSTM é¢„æµ‹å¤±è´¥: {e}\")\n",
    "\n",
    "    # ==================== 4. ç±»åˆ«é”€å”®å¯è§†åŒ– ====================\n",
    "    print(\"\\n--- ğŸ›ï¸ æ­£åœ¨ç”Ÿæˆå“ç±»è¡¨ç°å›¾ ---\")\n",
    "    category_means = noné—_products.groupby('Category')['Amount'].mean().sort_values(ascending=False).reset_index()\n",
    "    fig_bar = px.bar(category_means, x='Category', y='Amount', color='Category', text_auto='.2f', title='å„äº§å“ç±»åˆ«å¹³å‡é”€å”®é¢å¯¹æ¯”')\n",
    "    fig_bar.update_layout(width=800, height=500, showlegend=False); fig_bar.show()\n",
    "\n",
    "    # ==================== 5. å•†å“èšç±»åˆ†æ ====================\n",
    "    print(\"\\n--- ğŸ”¥ æ­£åœ¨è¿›è¡Œå•†å“èšç±»åˆ†æ ---\")\n",
    "    try:\n",
    "        product_agg_df = amazon_df.groupby('SKU').agg(total_amount=('Amount', 'sum'), total_qty=('Qty', 'sum'), order_count=('Order ID', 'nunique')).reset_index()\n",
    "        features_to_cluster = ['total_amount', 'total_qty', 'order_count']; features = product_agg_df[features_to_cluster]\n",
    "        scaler = StandardScaler(); features_scaled = scaler.fit_transform(features)\n",
    "        kmeans = KMeans(n_clusters=3, n_init=10, random_state=42); product_agg_df.loc[:, 'cluster'] = kmeans.fit_predict(features_scaled)\n",
    "        cluster_summary = product_agg_df.groupby('cluster')[features_to_cluster].mean().sort_values(by='total_amount', ascending=False)\n",
    "        hot_product_cluster_id = cluster_summary.index[0]\n",
    "        hot_products = product_agg_df[product_agg_df['cluster'] == hot_product_cluster_id].sort_values(by='total_amount', ascending=False)\n",
    "        print(\"\\næ¯ä¸ªå•†å“ç°‡çš„ç‰¹å¾å‡å€¼:\"); display(cluster_summary)\n",
    "        print(\"\\næ’åå‰10çš„çƒ­é”€å•†å“:\"); display(hot_products.head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å•†å“èšç±»å¤±è´¥: {e}\")\n",
    "\n",
    "    # ==================== 6. æƒ…æ„Ÿåˆ†æ ====================\n",
    "    if reviews_file:\n",
    "        print(\"\\n--- ğŸ’¬ æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ ---\")\n",
    "        try:\n",
    "            def find_review_column(df):\n",
    "                priority_cols = ['reviews.text', 'review_text', 'content', 'comment', 'review']\n",
    "                for p_col in priority_cols:\n",
    "                    if p_col in df.columns and df[p_col].dropna().astype(str).str.strip().any(): return p_col\n",
    "                object_cols = df.select_dtypes(include=['object']).columns\n",
    "                if not object_cols.empty:\n",
    "                    return max(object_cols, key=lambda col: df[col].dropna().astype(str).str.len().mean())\n",
    "                return None\n",
    "            reviews_df = pd.read_csv(reviews_file)\n",
    "            review_column_name = find_review_column(reviews_df)\n",
    "            if review_column_name is None: raise ValueError(\"æœªèƒ½è‡ªåŠ¨æ£€æµ‹åˆ°æ–‡æœ¬åˆ—ã€‚\")\n",
    "            reviews_df.rename(columns={review_column_name: 'review_text'}, inplace=True)\n",
    "            reviews_df.dropna(subset=['review_text'], inplace=True)\n",
    "            analyzer = SentimentIntensityAnalyzer()\n",
    "            reviews_df['sentiment'] = reviews_df['review_text'].apply(lambda text: analyzer.polarity_scores(str(text))['compound'])\n",
    "            if 'rating' not in reviews_df.columns:\n",
    "                reviews_df['rating'] = reviews_df['sentiment'].apply(sentiment_to_rating)\n",
    "            print(\"\\næƒ…æ„Ÿåˆ†æç»“æœé¢„è§ˆ:\"); display(reviews_df.head())\n",
    "            print(\"\\né«˜åˆ†(>=4æ˜Ÿ)ä¸ä½åˆ†(<=2æ˜Ÿ)è¯„è®ºå¯¹æ¯”:\"); display(reviews_df[reviews_df['rating'] >= 4].head(3)); display(reviews_df[reviews_df['rating'] <= 2].head(3))\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}\")\n",
    "\n",
    "    # ==================== 7. å¤šè¯­è¨€ç¿»è¯‘ ====================\n",
    "    if unesco_file and 'unesco_df' in locals():\n",
    "        print(\"\\n--- ğŸŒ æ­£åœ¨è¿›è¡Œéé—æè¿°ç¿»è¯‘ (ä»…å‰5æ¡ä½œä¸ºæ¼”ç¤º) ---\")\n",
    "        try:\n",
    "            def translate_text(text, target_lang):\n",
    "                if not isinstance(text, str) or not text.strip(): return \"\"\n",
    "                try: return GoogleTranslator(source='auto', target=target_lang).translate(text)\n",
    "                except: return text\n",
    "            target_languages = ['de', 'fr', 'zh-cn']; unesco_translated_df = unesco_df.head(5).copy()\n",
    "            for lang in target_languages:\n",
    "                column_name = f'Description_{lang.upper()}'\n",
    "                unesco_translated_df[column_name] = unesco_translated_df['Description EN'].apply(lambda x: translate_text(x, lang))\n",
    "            print(\"\\nç¿»è¯‘å®Œæˆåçš„æ•°æ®é¢„è§ˆï¼š\"); display(unesco_translated_df)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç¿»è¯‘å¤±è´¥: {e}\")\n",
    "            \n",
    "    # ==================== 8. (æ–°å¢) è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é… ====================\n",
    "    if unesco_file and not relevant_unesco.empty:\n",
    "        print(\"\\n--- ğŸ”— æ­£åœ¨è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é… ---\")\n",
    "        try:\n",
    "            cosine_sim, tfidf_vectorizer, unesco_titles = calculate_similarity(amazon_df, relevant_unesco)\n",
    "            print(\"âœ… ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆï¼\")\n",
    "            print(f\"--> ç”Ÿæˆçš„ç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {cosine_sim.shape} (å•†å“æ•°, éé—é¡¹ç›®æ•°)\")\n",
    "            print(\"\\n--- åœºæ™¯1: ä¸ºçƒ­é”€å•†å“åŒ¹é…æ–‡åŒ–çµæ„Ÿ ---\")\n",
    "            if 'hot_products' in locals() and not hot_products.empty:\n",
    "                product_sku_to_test = hot_products.iloc[0]['SKU']\n",
    "                product_indices = amazon_df.index[amazon_df['SKU'] == product_sku_to_test].tolist()\n",
    "                if product_indices:\n",
    "                    product_idx = product_indices[0]\n",
    "                    sim_scores = list(enumerate(cosine_sim[product_idx]))\n",
    "                    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "                    top_5_indices = [i[0] for i in sim_scores[0:5]]\n",
    "                    print(f\"ä¸æœ€çƒ­é”€å•†å“ '{product_sku_to_test}' æœ€ç›¸ä¼¼çš„å‰5ä¸ªéé—é¡¹ç›®æ˜¯:\")\n",
    "                    for idx in top_5_indices:\n",
    "                        print(f\"- {unesco_titles[idx]} (ç›¸ä¼¼åº¦: {cosine_sim[product_idx, idx]:.4f})\")\n",
    "                else:\n",
    "                    print(f\"æœªèƒ½åœ¨å¤§è¡¨ä¸­å®šä½åˆ°çƒ­é”€å•†å“ '{product_sku_to_test}'ï¼Œè·³è¿‡åœºæ™¯1ã€‚\")\n",
    "            else:\n",
    "                print(\"æœªæ‰¾åˆ°çƒ­é”€å•†å“åˆ—è¡¨ (å¯èƒ½èšç±»å¤±è´¥)ï¼Œè·³è¿‡åœºæ™¯1ã€‚\")\n",
    "\n",
    "            print(\"\\n--- åœºæ™¯2: æ ¹æ®æ–‡åŒ–å…ƒç´ åå‘å¯»æ‰¾æ½œåŠ›å•†å“ ---\")\n",
    "            if unesco_titles:\n",
    "                heritage_idx = np.random.randint(0, len(unesco_titles))\n",
    "                heritage_title_to_test = unesco_titles[heritage_idx]\n",
    "                sim_scores_for_heritage = list(enumerate(cosine_sim[:, heritage_idx]))\n",
    "                sim_scores_for_heritage = sorted(sim_scores_for_heritage, key=lambda x: x[1], reverse=True)\n",
    "                top_10_product_indices = [i[0] for i in sim_scores_for_heritage[0:10]]\n",
    "                print(f\"\\nä¸éé—é¡¹ç›® '{heritage_title_to_test}' æœ€ç›¸ä¼¼çš„å‰10ä¸ªåœ¨å”®å•†å“æ˜¯:\")\n",
    "                recommended_products = amazon_df.iloc[top_10_product_indices]\n",
    "                display(recommended_products[['SKU', 'Amount', 'Category']])\n",
    "            else:\n",
    "                print(\"æ— å¯ç”¨çš„éé—é¡¹ç›®æ ‡é¢˜ï¼Œè·³è¿‡åœºæ™¯2ã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…å¤±è´¥: {e}\")\n",
    "    else:\n",
    "        print(\"\\n--- ğŸ”— è·³è¿‡è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ï¼šæœªæä¾›UNESCOæ–‡ä»¶æˆ–æœªæ‰¾åˆ°ç›¸å…³çš„éé—é¡¹ç›® ---\")\n",
    "            \n",
    "    print(\"\\n--- âœ¨ åˆ†æå…¨éƒ¨å®Œæˆ ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03325421-bc8d-4fcf-a38e-fa46d0e43c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8d4ecc4114499caba00631faad8627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='é”€å”®æ–‡ä»¶:', options=('data\\\\Amazon Sale Report.csv', 'data\\\\amazon-fashion-80â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: åˆ›å»ºå¹¶æ˜¾ç¤ºäº¤äº’å¼ç•Œé¢\n",
    "# ==============================================================================\n",
    "import os\n",
    "from ipywidgets import interactive_output, Dropdown, VBox\n",
    "\n",
    "data_folder = 'data'\n",
    "\n",
    "if not os.path.isdir(data_folder):\n",
    "    print(f\"é”™è¯¯ï¼šè¯·å…ˆåˆ›å»ºåä¸º '{data_folder}' çš„æ–‡ä»¶å¤¹ï¼Œå¹¶å°†æ‰€æœ‰æ•°æ®æ–‡ä»¶æ”¾å…¥å…¶ä¸­ã€‚\")\n",
    "else:\n",
    "    all_data_files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) \n",
    "                      if f.endswith(('.csv', '.xls', 'xlsx'))]\n",
    "\n",
    "    sales_report_options = [f for f in all_data_files if 'amazon' in f.lower() or 'sales' in f.lower()]\n",
    "    unesco_options = [f for f in all_data_files if 'ich' in f.lower() or 'unesco' in f.lower()]\n",
    "    reviews_options = [None] + [f for f in all_data_files if 'review' in f.lower()]\n",
    "    metadata_options = [None] + all_data_files\n",
    "\n",
    "    if not sales_report_options or not unesco_options:\n",
    "        print(f\"é”™è¯¯ï¼šè¯·ç¡®ä¿é”€å”®æ–‡ä»¶å’ŒUNESCOæ–‡ä»¶éƒ½åœ¨ '{data_folder}' ç›®å½•ä¸­ã€‚\")\n",
    "    else:\n",
    "        # åˆ›å»ºæ‰€æœ‰æ§ä»¶\n",
    "        sales_dropdown = Dropdown(options=sales_report_options, description='é”€å”®æ–‡ä»¶:')\n",
    "        metadata_dropdown = Dropdown(options=metadata_options, description='å…ƒæ•°æ®æ–‡ä»¶ (å¯é€‰):')\n",
    "        unesco_dropdown = Dropdown(options=unesco_options, description='UNESCOæ–‡ä»¶:')\n",
    "        reviews_dropdown = Dropdown(options=reviews_options, description='è¯„è®ºæ–‡ä»¶ (å¯é€‰):')\n",
    "\n",
    "        output_area = widgets.Output()\n",
    "\n",
    "        def on_value_change(change):\n",
    "            with output_area:\n",
    "                run_analysis(sales_dropdown.value, unesco_dropdown.value, reviews_dropdown.value, metadata_dropdown.value)\n",
    "\n",
    "        # ç›‘å¬æ‰€æœ‰æ§ä»¶çš„å˜åŒ–\n",
    "        sales_dropdown.observe(on_value_change, names='value')\n",
    "        metadata_dropdown.observe(on_value_change, names='value')\n",
    "        unesco_dropdown.observe(on_value_change, names='value')\n",
    "        reviews_dropdown.observe(on_value_change, names='value')\n",
    "\n",
    "        # å°†æ‰€æœ‰æ§ä»¶å’Œè¾“å‡ºåŒºåŸŸä¸€èµ·æ˜¾ç¤ºå‡ºæ¥\n",
    "        display(VBox([sales_dropdown, metadata_dropdown, unesco_dropdown, reviews_dropdown, output_area]))\n",
    "        \n",
    "        # é¦–æ¬¡æ‰‹åŠ¨è§¦å‘è¿è¡Œ\n",
    "        on_value_change(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df3b77-595b-4542-811f-37903e69992c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
